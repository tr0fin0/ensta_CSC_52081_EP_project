{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC_52081_EP Project\n",
    "\n",
    "Advanced Machine Learning and Autonomous Agents Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) has emerged as a robust framework for training autonomous agents to learn optimal behaviors through environmental interactions. This study utilizes the [`CarRacing-v3`](https://gymnasium.farama.org/environments/box2d/car_racing/) environment from Gymnasium, which presents a challenging control task in a racing scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment features a high-dimensional observation space, represented by a $96 \\times 96$ RGB image capturing the car and track, necessitating the use of deep convolutional neural networks (CNNs) for effective feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space in CarRacing-v3 supports both continuous and discrete control modes.\n",
    "\n",
    "In **continuous mode**, the agent outputs three real-valued commands:\n",
    "\n",
    "- steering (ranging from $-1$ to $+1$)\n",
    "- gas\n",
    "- braking\n",
    "\n",
    "In **discrete mode**, the action space is simplified to five actions:\n",
    "\n",
    "- do nothing\n",
    "- steer left\n",
    "- steer right\n",
    "- gas\n",
    "- brake\n",
    "\n",
    "This dual action representation enables a comprehensive evaluation of various RL algorithms under different control settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward structure combines a penalty of $-0.1$ per frame and a reward of $+\\frac{1000}{N}$ for each new track tile visited, where $N$ is the total number of tiles. This incentivizes the agent to balance exploration (visiting tiles) with efficiency (minimizing frame usage). For example, completing the race after visiting all $N$ tiles in 732 frames yields a reward of $1000 - 0.1 \\times 732 = 926.8$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of this project is to compare RL policies across discrete and continuous action modalities. For discrete control, methods like **Deep Q-Network** (DQN) and **SARSA** are implemented, while continuous control is explored using approaches such as the **Cross-Entropy Method** (CEM), **Self-Adaptive Evolution Strategy** (SA-ES), and policy gradient techniques like **Proximal Policy Optimization** (PPO) and **Soft Actor-Critic** (SAC). This comparative analysis aims to understand the strengths and limitations of each method in handling complex decision spaces.\n",
    "\n",
    "The high-dimensional visual inputs in `CarRacing-v3` require effective feature extraction, addressed through a tailored CNN architecture. Transitioning between discrete and continuous action representations also demands careful algorithmic design and parameter tuning to ensure stable learning and convergence. While prior studies have often focused on either discrete or continuous action spaces separately, this work adopts a comparative approach, evaluating different agents within the same environment to assess performance under similar conditions.\n",
    "\n",
    "At this stage, the work outlines the methodology and anticipated challenges, focusing on designing the CNN-based feature extractor, implementing RL algorithms, and establishing a framework for performance comparison. Preliminary findings are yet to be finalized, but the study is expected to provide insights into applying RL in high-dimensional, real-time control tasks. Limitations include the preliminary nature of experiments and the need for further tuning and validation. Future work will involve extensive empirical evaluations, exploring additional policy gradient methods, and refining the network architecture to better handle the complexities of `CarRacing-v3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project's code is available on [GitHub](https://github.com/tr0fin0/ensta_CSC_52081_EP_project), offering a reproducible framework for future investigations and extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WSL, Linux or MacOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Python Virtual Environment` will be used for this project by run the following on a terminal on the project folder:\n",
    "\n",
    "```bash\n",
    "sudo apt install python3.10-venv\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Video\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import cast, List, Tuple, Deque, Optional, Callable\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_selector(file_path: List[Path]) -> Video:\n",
    "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_OUTPUT = \"output\"\n",
    "DIRECTORY_MODELS = Path(f\"{DIRECTORY_OUTPUT}/models/\")\n",
    "DIRECTORY_FIGURES = Path(f\"{DIRECTORY_OUTPUT}/images/\")\n",
    "\n",
    "if not DIRECTORY_FIGURES.exists():\n",
    "    DIRECTORY_FIGURES.mkdir(parents=True)\n",
    "\n",
    "if not DIRECTORY_MODELS.exists():\n",
    "    DIRECTORY_MODELS.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidAction",
     "evalue": "you passed the invalid action `1.0`. The supported action_space is `Discrete(5)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidAction\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     19\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 21\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m     24\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:350\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     obs, rew, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_trigger \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_trigger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id):\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\gymnasium\\wrappers\\common.py:283\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:207\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    209\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    210\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:551\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(action):\n\u001b[1;32m--> 551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidAction(\n\u001b[0;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou passed the invalid action `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe supported action_space is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39msteer(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.6\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.6\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mgas(\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[1;31mInvalidAction\u001b[0m: you passed the invalid action `1.0`. The supported action_space is `Discrete(5)`"
     ]
    }
   ],
   "source": [
    "VIDEO_DEMO = \"CSC_52081_EP_demonstration\"\n",
    "(DIRECTORY_FIGURES / f\"{VIDEO_DEMO}.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    lap_complete_percent=0.95,\n",
    "    domain_randomize=False,\n",
    "    continuous=False\n",
    ")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=str(DIRECTORY_FIGURES), name_prefix=VIDEO_DEMO)\n",
    "\n",
    "\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "Video(\n",
    "    DIRECTORY_FIGURES / f\"{VIDEO_DEMO}-episode-0.mp4\",\n",
    "    embed=True,\n",
    "    html_attributes=\"controls autoplay loop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only demonstration is right. from below here is only experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU\n",
    "\n",
    "STATE_SHAPE = (96, 96, 3)\n",
    "DISCRETE_ACTIONS = 5\n",
    "CONTINUOUS_ACTIONS = 3\n",
    "EPISODES = 1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_action_heatmap(action_counts):\n",
    "    plt.imshow(action_counts, cmap=\"hot\", interpolation=\"nearest\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Action Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "def test_agent(\n",
    "    env: gym.Env, agent: torch.nn.Module, num_episode: int = 1\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Test a naive agent in the given environment using the provided Q-network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to test the agent.\n",
    "    agent : torch.nn.Module\n",
    "        The Q-network to use for decision making.\n",
    "    num_episode : int, optional\n",
    "        The number of episodes to run, by default 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_id in range(num_episode):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "        action = agent(state)\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "            state, reward, done, terminated, truncated = env.step(action)\n",
    "            state = torch.tensor(state, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "\n",
    "            # Compute the Q-values for the current state using the Q-network\n",
    "            q_values = agent(state)\n",
    "\n",
    "            # Select the action with the highest Q-value\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "            # Update the episode reward\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium.wrappers as gym_wrap\n",
    "import os\n",
    "\n",
    "def save(self, save_dir, save_name):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = save_dir + save_name + f\"_{self.act_taken}.pt\"\n",
    "    torch.save({\n",
    "        'upd_model_state_dict': self.updating_net.state_dict(),\n",
    "        'frz_model_state_dict': self.frozen_net.state_dict(),\n",
    "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        # 'replay_buffer': self.buffer,  # Exclude replay buffer from saving\n",
    "        'action_number': self.act_taken,\n",
    "        'epsilon': self.epsilon\n",
    "        }, save_path)\n",
    "    print(f\"Model saved to {save_path} at step {self.act_taken}\")\n",
    "\n",
    "def load(self, load_dir, model_name):\n",
    "    loaded_model = torch.load(load_dir+model_name, weights_only=False)\n",
    "    upd_net_param = loaded_model['upd_model_state_dict']\n",
    "    frz_net_param = loaded_model['frz_model_state_dict']\n",
    "    opt_param = loaded_model['optimizer_state_dict']\n",
    "    self.updating_net.load_state_dict(upd_net_param)\n",
    "    self.frozen_net.load_state_dict(frz_net_param)\n",
    "    self.optimizer.load_state_dict(opt_param)\n",
    "    if self.load_state == 'eval':\n",
    "        self.updating_net.eval()\n",
    "        self.frozen_net.eval()\n",
    "        self.epsilon_min = 0\n",
    "        self.epsilon = 0\n",
    "    elif self.load_state == 'train':\n",
    "        self.updating_net.train()\n",
    "        self.frozen_net.train()\n",
    "        self.act_taken = loaded_model['action_number']\n",
    "        self.epsilon = loaded_model['epsilon']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown load state. Should be either 'eval' or 'train'.\")\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return state, total_reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        channel_n, height, width = in_dim\n",
    "\n",
    "        if height != 84 or width != 84:\n",
    "            raise ValueError(f\"DQN model requires input of a (84, 84)-shape. Input of a ({height, width})-shape was passed.\")\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channel_n, out_channels=16,\n",
    "                      kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32,\n",
    "                      kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2592, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    continuous=False\n",
    ")\n",
    "env = DQN.SkipFrame(env, skip=4)\n",
    "env = gym_wrap.GrayscaleObservation(env)\n",
    "env = gym_wrap.ResizeObservation(env, shape=(84, 84))\n",
    "env = gym_wrap.FrameStackObservation(env, stack_size=4)\n",
    "state, info = env.reset()\n",
    "action_n = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Parameters\n",
    "EPISODES = 1000  # Training episodes\n",
    "GAMMA = 0.99  # Discount factor\n",
    "ALPHA = 0.001  # Learning rate\n",
    "EPSILON = 1.0  # Exploration rate\n",
    "EPSILON_DECAY = 0.995  # Decay factor\n",
    "EPSILON_MIN = 0.05  # Minimum exploration rate\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return state, total_reward, terminated, truncated, info\n",
    "\n",
    "# CNN-based Q-network\n",
    "class DeepSARSA(nn.Module):\n",
    "    def __init__(self, state_shape, action_size):\n",
    "        super().__init__()\n",
    "        channel_n, height, width = state_shape\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channel_n, out_channels=16,\n",
    "                      kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32,\n",
    "                      kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2592, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Training function\n",
    "def train_deep_sarsa(model: nn.Module = None):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=ALPHA)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    memory = deque(maxlen=10000)\n",
    "    \n",
    "    global EPSILON\n",
    "    for episode in range(EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n",
    "        action = select_action(model, state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n",
    "            next_action = select_action(model, next_state)  # SARSA selects next action from current policy\n",
    "            done = truncated or terminated\n",
    "\n",
    "            memory.append((state, action, reward, next_state, next_action, done))\n",
    "            state, action = next_state, next_action\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update model\n",
    "            if len(memory) > 32:\n",
    "                replay_experience(model, optimizer, loss_fn, memory, device)\n",
    "\n",
    "        EPSILON = max(EPSILON * EPSILON_DECAY, EPSILON_MIN)  # Decay exploration\n",
    "        print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Action selection using Îµ-greedy policy\n",
    "def select_action(model, state):\n",
    "    if random.random() < EPSILON:\n",
    "        return random.randint(0, DISCRETE_ACTIONS - 1)  # Random action\n",
    "    with torch.no_grad():\n",
    "        return torch.argmax(model(state)).item()  # Best action from Q-network\n",
    "\n",
    "# Experience replay function\n",
    "def replay_experience(model, optimizer, loss_fn, memory, device):\n",
    "    batch = random.sample(memory, 32)\n",
    "    \n",
    "    states, actions, rewards, next_states, next_actions, dones = zip(*batch)\n",
    "    states = torch.cat(states).to(device)\n",
    "    next_states = torch.cat(next_states).to(device)\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    next_actions = torch.tensor(next_actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    \n",
    "    q_values = model(states).gather(1, actions)\n",
    "    next_q_values = model(next_states).gather(1, next_actions).detach()\n",
    "\n",
    "    target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m gym_wrap\u001b[38;5;241m.\u001b[39mResizeObservation(env, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m))\n\u001b[0;32m      7\u001b[0m env \u001b[38;5;241m=\u001b[39m gym_wrap\u001b[38;5;241m.\u001b[39mFrameStackObservation(env, stack_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m state_shape \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m discrete_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    continuous=False\n",
    ")\n",
    "env = gym_wrap.GrayscaleObservation(env)\n",
    "env = gym_wrap.ResizeObservation(env, shape=(84, 84))\n",
    "env = gym_wrap.FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "state_shape = env.observation_space.shape()\n",
    "discrete_actions = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run Deep SARSA training\n",
    "model = DeepSARSA(state_shape, discrete_actions).to(device)\n",
    "model = train_deep_sarsa(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 3\n",
    "\n",
    "FIGS_DIR = Path(\"figs/\")       # Where to save figures (.gif files)\n",
    "\n",
    "VIDEO_DIRNAME = \"deep_sarsa\"\n",
    "\n",
    "(FIGS_DIR / VIDEO_DIRNAME / \"ds-video-episode-0.mp4\").unlink(missing_ok=True)\n",
    "(FIGS_DIR / VIDEO_DIRNAME / \"ds-video-episode-1.mp4\").unlink(missing_ok=True)\n",
    "(FIGS_DIR / VIDEO_DIRNAME / \"ds-video-episode-2.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=FIGS_DIR / VIDEO_DIRNAME, episode_trigger=lambda x: True)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    total_reward = 0.0\n",
    "    state, info = env.reset()\n",
    "    action = model(state)\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        #state = torch.tensor(state, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "        action = model(state)\n",
    "        total_reward += reward\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f'Episode time taken: {env.time_queue}')\n",
    "print(f'Episode total rewards: {env.return_queue}')\n",
    "print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"ds-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"ds-video-episode-1.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"ds-video-episode-2.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
