{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC_52081_EP Project\n",
    "\n",
    "Advanced Machine Learning and Autonomous Agents Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) has emerged as a robust framework for training autonomous agents to learn optimal behaviors through environmental interactions. This study utilizes the [`CarRacing-v3`](https://gymnasium.farama.org/environments/box2d/car_racing/) environment from Gymnasium, which presents a challenging control task in a racing scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment features a high-dimensional observation space, represented by a $96 \\times 96$ RGB image capturing the car and track, necessitating the use of deep convolutional neural networks (CNNs) for effective feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space in CarRacing-v3 supports both continuous and discrete control modes.\n",
    "\n",
    "In **continuous mode**, the agent outputs three real-valued commands:\n",
    "\n",
    "- steering (ranging from $-1$ to $+1$)\n",
    "- gas\n",
    "- braking\n",
    "\n",
    "In **discrete mode**, the action space is simplified to five actions:\n",
    "\n",
    "- do nothing\n",
    "- steer left\n",
    "- steer right\n",
    "- gas\n",
    "- brake\n",
    "\n",
    "This dual action representation enables a comprehensive evaluation of various RL algorithms under different control settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward structure combines a penalty of $-0.1$ per frame and a reward of $+\\frac{1000}{N}$ for each new track tile visited, where $N$ is the total number of tiles. This incentivizes the agent to balance exploration (visiting tiles) with efficiency (minimizing frame usage). For example, completing the race after visiting all $N$ tiles in 732 frames yields a reward of $1000 - 0.1 \\times 732 = 926.8$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of this project is to compare RL policies across discrete and continuous action modalities. For discrete control, methods like **Deep Q-Network** (DQN) and **SARSA** are implemented, while continuous control is explored using approaches such as the **Cross-Entropy Method** (CEM), **Self-Adaptive Evolution Strategy** (SA-ES), and policy gradient techniques like **Proximal Policy Optimization** (PPO) and **Soft Actor-Critic** (SAC). This comparative analysis aims to understand the strengths and limitations of each method in handling complex decision spaces.\n",
    "\n",
    "The high-dimensional visual inputs in `CarRacing-v3` require effective feature extraction, addressed through a tailored CNN architecture. Transitioning between discrete and continuous action representations also demands careful algorithmic design and parameter tuning to ensure stable learning and convergence. While prior studies have often focused on either discrete or continuous action spaces separately, this work adopts a comparative approach, evaluating different agents within the same environment to assess performance under similar conditions.\n",
    "\n",
    "At this stage, the work outlines the methodology and anticipated challenges, focusing on designing the CNN-based feature extractor, implementing RL algorithms, and establishing a framework for performance comparison. Preliminary findings are yet to be finalized, but the study is expected to provide insights into applying RL in high-dimensional, real-time control tasks. Limitations include the preliminary nature of experiments and the need for further tuning and validation. Future work will involve extensive empirical evaluations, exploring additional policy gradient methods, and refining the network architecture to better handle the complexities of `CarRacing-v3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project's code is available on [GitHub](https://github.com/tr0fin0/ensta_CSC_52081_EP_project), offering a reproducible framework for future investigations and extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WSL, Linux or MacOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Python Virtual Environment` will be used for this project by run the following on a terminal on the project folder:\n",
    "\n",
    "```bash\n",
    "sudo apt install python3.10-venv\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "plt.ion()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "\n",
    "from IPython.display import Video\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "DIRECTORY_OUTPUT = \"output\"\n",
    "DIRECTORY_MODELS = Path(f\"{DIRECTORY_OUTPUT}/models/\")\n",
    "DIRECTORY_FIGURES = Path(f\"{DIRECTORY_OUTPUT}/images/\")\n",
    "DIRECTORY_LOGS = Path(f\"{DIRECTORY_OUTPUT}/logs/\")\n",
    "\n",
    "for directory in [DIRECTORY_MODELS, DIRECTORY_FIGURES, DIRECTORY_LOGS]:\n",
    "    if not directory.exists():\n",
    "        directory.mkdir(parents=True)\n",
    "\n",
    "print(DIRECTORY_OUTPUT)\n",
    "print(DIRECTORY_MODELS)\n",
    "print(DIRECTORY_FIGURES)\n",
    "print(DIRECTORY_LOGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DEMO = \"CSC_52081_EP_demonstration\"\n",
    "(DIRECTORY_FIGURES / f\"{VIDEO_DEMO}.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    lap_complete_percent=0.95,\n",
    "    domain_randomize=False,\n",
    "    continuous=False\n",
    ")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=str(DIRECTORY_FIGURES), name_prefix=VIDEO_DEMO)\n",
    "\n",
    "\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "Video(\n",
    "    DIRECTORY_FIGURES / f\"{VIDEO_DEMO}-episode-0.mp4\",\n",
    "    embed=True,\n",
    "    html_attributes=\"controls autoplay loop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only demonstration is right. from below here is only experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actions \u001b[38;5;241m%\u001b[39m interval_learn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     qf1_loss, qf2_loss, policy_loss, alpha_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCHES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend((qf1_loss, qf2_loss, policy_loss, alpha_loss))\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actions \u001b[38;5;241m%\u001b[39m interval_save \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/git_repositories/classes_ensta/3A/CSC_52081_EP/CSC_52081_EP_project/src/dev/Agent_SAC_7.py:283\u001b[0m, in \u001b[0;36mAgentSAC.update_network\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    280\u001b[0m alpha_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (log_pi \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_entropy)\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 283\u001b[0m \u001b[43malpha_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Retain graph for alpha loss\u001b[39;00m\n\u001b[1;32m    284\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha], max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/git_repositories/classes_ensta/3A/CSC_52081_EP/CSC_52081_EP_project/src/env/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_repositories/classes_ensta/3A/CSC_52081_EP/CSC_52081_EP_project/src/env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_repositories/classes_ensta/3A/CSC_52081_EP/CSC_52081_EP_project/src/env/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "plt.ion()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "DIRECTORY_OUTPUT = \"output\"\n",
    "DIRECTORY_MODELS = Path(f\"{DIRECTORY_OUTPUT}/models/\")\n",
    "DIRECTORY_FIGURES = Path(f\"{DIRECTORY_OUTPUT}/images/\")\n",
    "DIRECTORY_LOGS = Path(f\"{DIRECTORY_OUTPUT}/logs/\")\n",
    "\n",
    "for directory in [DIRECTORY_MODELS, DIRECTORY_FIGURES, DIRECTORY_LOGS]:\n",
    "    if not directory.exists():\n",
    "        directory.mkdir(parents=True)\n",
    "\n",
    "\n",
    "from Agent_SAC import AgentSAC\n",
    "from CNN import CNN\n",
    "from utils import get_environment, plot_reward\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_LOAD = False\n",
    "MODEL_ID = 400000\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCHES = 64\n",
    "EPISODES = 3000\n",
    "EPISODES_EXTRA = 300\n",
    "\n",
    "# Use a continuous environment for SAC\n",
    "env_continuous = get_environment(is_continuous=True)\n",
    "state, info = env_continuous.reset()\n",
    "\n",
    "agent = AgentSAC(\n",
    "    state.shape,\n",
    "    env_continuous.action_space.shape,\n",
    "    DEVICE,\n",
    "    DIRECTORY_MODELS,\n",
    "    DIRECTORY_LOGS,\n",
    "    CNN,\n",
    "    policy_type=\"Gaussian\",  # or \"Deterministic\" if preferred\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    lr=3e-5,\n",
    "    alpha=0.2,\n",
    "    auto_alpha_tuning=True,\n",
    "    batch_size=BATCHES,\n",
    "    hidden_size=256,\n",
    "    target_update_interval=1,\n",
    "    load_state=MODEL_LOAD,\n",
    "    load_model=f'SAC_{MODEL_ID}.pt' if MODEL_LOAD else None,\n",
    ")\n",
    "\n",
    "episode = EPISODES if MODEL_LOAD else 0\n",
    "actions = MODEL_ID if MODEL_LOAD else 0\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "episode_losses = []\n",
    "episode_dates = []\n",
    "episode_times = []\n",
    "\n",
    "interval_learn = 4      # [actions]\n",
    "interval_save = 10000   # [actions]\n",
    "interval_log = 10       # [episodes]\n",
    "\n",
    "while episode <= EPISODES + (EPISODES_EXTRA if MODEL_LOAD else 0):\n",
    "    episode += 1\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "\n",
    "    done = False\n",
    "    losses = []\n",
    "\n",
    "    while not done:\n",
    "        actions += 1\n",
    "        episode_length += 1\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        if np.isnan(action).any():\n",
    "            action = env_continuous.action_space.sample()  # Sample a random action if the action is NaN\n",
    "        next_state, reward, terminated, truncated, info = env_continuous.step(action)\n",
    "\n",
    "        # print(action, terminated, truncated)\n",
    "\n",
    "        episode_reward += reward\n",
    "        agent.store(state, action, reward, next_state, terminated)\n",
    "\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if actions % interval_learn == 0:\n",
    "            qf1_loss, qf2_loss, policy_loss, alpha_loss = agent.update_network(BATCHES)\n",
    "            losses.append((qf1_loss, qf2_loss, policy_loss, alpha_loss))\n",
    "\n",
    "        if actions % interval_save == 0:\n",
    "            agent.save()\n",
    "\n",
    "    state, info = env_continuous.reset()\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    episode_losses.append(np.mean(losses, axis=0))\n",
    "    now_time = datetime.datetime.now()\n",
    "    episode_dates.append(now_time.date().strftime('%Y-%m-%d'))\n",
    "    episode_times.append(now_time.time().strftime('%H:%M:%S'))\n",
    "\n",
    "    plot_reward(episode, episode_rewards, actions)\n",
    "\n",
    "    if episode % interval_log == 0:\n",
    "        agent.write_log(\n",
    "            episode_dates,\n",
    "            episode_times,\n",
    "            episode_rewards,\n",
    "            episode_lengths,\n",
    "            episode_losses,\n",
    "        )\n",
    "\n",
    "agent.save()\n",
    "agent.write_log(\n",
    "    episode_dates,\n",
    "    episode_times,\n",
    "    episode_rewards,\n",
    "    episode_lengths,\n",
    "    episode_losses,\n",
    ")\n",
    "env_continuous.close()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = 2500\n",
    "SEEDS = [i for i in range(2)]\n",
    "\n",
    "\n",
    "env_continuous = get_environment_continuous()\n",
    "state, info = env_continuous.reset()\n",
    "\n",
    "agent = Agent_SAC(\n",
    "    state.shape,\n",
    "    env_continuous.action_space,\n",
    "    DEVICE,\n",
    "    DIRECTORY_MODELS,\n",
    "    DIRECTORY_LOGS,\n",
    "    load_state='eval',\n",
    "    load_model= f'SAC_{MODEL_ID}.pt',\n",
    ")\n",
    "\n",
    "agent_evaluation(agent, env_continuous, SEEDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
