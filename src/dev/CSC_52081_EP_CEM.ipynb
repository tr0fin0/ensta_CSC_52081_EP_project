{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC_52081_EP Project\n",
    "\n",
    "Advanced Machine Learning and Autonomous Agents Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) has emerged as a robust framework for training autonomous agents to learn optimal behaviors through environmental interactions. This study utilizes the [`CarRacing-v3`](https://gymnasium.farama.org/environments/box2d/car_racing/) environment from Gymnasium, which presents a challenging control task in a racing scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment features a high-dimensional observation space, represented by a $96 \\times 96$ RGB image capturing the car and track, necessitating the use of deep convolutional neural networks (CNNs) for effective feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space in CarRacing-v3 supports both continuous and discrete control modes.\n",
    "\n",
    "In **continuous mode**, the agent outputs three real-valued commands:\n",
    "\n",
    "- steering (ranging from $-1$ to $+1$)\n",
    "- gas\n",
    "- braking\n",
    "\n",
    "In **discrete mode**, the action space is simplified to five actions:\n",
    "\n",
    "- do nothing\n",
    "- steer left\n",
    "- steer right\n",
    "- gas\n",
    "- brake\n",
    "\n",
    "This dual action representation enables a comprehensive evaluation of various RL algorithms under different control settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward structure combines a penalty of $-0.1$ per frame and a reward of $+\\frac{1000}{N}$ for each new track tile visited, where $N$ is the total number of tiles. This incentivizes the agent to balance exploration (visiting tiles) with efficiency (minimizing frame usage). For example, completing the race after visiting all $N$ tiles in 732 frames yields a reward of $1000 - 0.1 \\times 732 = 926.8$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of this project is to compare RL policies across discrete and continuous action modalities. For discrete control, methods like **Deep Q-Network** (DQN) and **SARSA** are implemented, while continuous control is explored using approaches such as the **Cross-Entropy Method** (CEM), **Self-Adaptive Evolution Strategy** (SA-ES), and policy gradient techniques like **Proximal Policy Optimization** (PPO) and **Soft Actor-Critic** (SAC). This comparative analysis aims to understand the strengths and limitations of each method in handling complex decision spaces.\n",
    "\n",
    "The high-dimensional visual inputs in `CarRacing-v3` require effective feature extraction, addressed through a tailored CNN architecture. Transitioning between discrete and continuous action representations also demands careful algorithmic design and parameter tuning to ensure stable learning and convergence. While prior studies have often focused on either discrete or continuous action spaces separately, this work adopts a comparative approach, evaluating different agents within the same environment to assess performance under similar conditions.\n",
    "\n",
    "At this stage, the work outlines the methodology and anticipated challenges, focusing on designing the CNN-based feature extractor, implementing RL algorithms, and establishing a framework for performance comparison. Preliminary findings are yet to be finalized, but the study is expected to provide insights into applying RL in high-dimensional, real-time control tasks. Limitations include the preliminary nature of experiments and the need for further tuning and validation. Future work will involve extensive empirical evaluations, exploring additional policy gradient methods, and refining the network architecture to better handle the complexities of `CarRacing-v3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project's code is available on [GitHub](https://github.com/tr0fin0/ensta_CSC_52081_EP_project), offering a reproducible framework for future investigations and extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WSL, Linux or MacOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Python Virtual Environment` will be used for this project by run the following on a terminal on the project folder:\n",
    "\n",
    "```bash\n",
    "sudo apt install python3.10-venv\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import csv\n",
    "\n",
    "\n",
    "from IPython.display import Video\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_selector(file_path: List[Path]) -> Video:\n",
    "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_OUTPUT = \"output\"\n",
    "DIRECTORY_MODELS = Path(f\"{DIRECTORY_OUTPUT}/models/\")\n",
    "DIRECTORY_FIGURES = Path(f\"{DIRECTORY_OUTPUT}/images/\")\n",
    "DIRECTORY_LOGS = Path(f\"{DIRECTORY_OUTPUT}/logs/\")\n",
    "\n",
    "if not DIRECTORY_FIGURES.exists():\n",
    "    DIRECTORY_FIGURES.mkdir(parents=True)\n",
    "\n",
    "if not DIRECTORY_MODELS.exists():\n",
    "    DIRECTORY_MODELS.mkdir(parents=True)\n",
    "\n",
    "if not DIRECTORY_LOGS.exists():\n",
    "    DIRECTORY_LOGS.mkdir(parents=True)\n",
    "\n",
    "print(DIRECTORY_OUTPUT)\n",
    "print(DIRECTORY_MODELS)\n",
    "print(DIRECTORY_FIGURES)\n",
    "print(DIRECTORY_LOGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DEMO = \"CSC_52081_EP_demonstration\"\n",
    "(DIRECTORY_FIGURES / f\"{VIDEO_DEMO}.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    lap_complete_percent=0.95,\n",
    "    domain_randomize=False,\n",
    "    continuous=False\n",
    ")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=str(DIRECTORY_FIGURES), name_prefix=VIDEO_DEMO)\n",
    "\n",
    "\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "Video(\n",
    "    DIRECTORY_FIGURES / f\"{VIDEO_DEMO}-episode-0.mp4\",\n",
    "    embed=True,\n",
    "    html_attributes=\"controls autoplay loop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only demonstration is right. from below here is only experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "def plot_reward(generation, reward_list, sigmas):\n",
    "    \"\"\"\n",
    "    Plot the reward per generation and a moving average.\n",
    "\n",
    "    Args:\n",
    "        generation (int): Current generation number.\n",
    "        reward_list (list): List of best rewards per generation.\n",
    "        sigmas (list): List of sigma values per generation.\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    rewards_tensor = torch.tensor(reward_list, dtype=torch.float)\n",
    "\n",
    "    if len(rewards_tensor) >= 11:\n",
    "        eval_reward = torch.clone(rewards_tensor[-10:])\n",
    "        mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "        std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "        plt.clf()\n",
    "        plt.title(\n",
    "            f'Gen #{generation}: Best Reward: {reward_list[-1]:.2f}, Sigma: {sigmas[-1]:.4f}, '\n",
    "            f'[{mean_eval_reward:.1f}±{std_eval_reward:.1f}]'\n",
    "        )\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_tensor.numpy())\n",
    "\n",
    "    if len(rewards_tensor) >= 50:\n",
    "        reward_f = torch.clone(rewards_tensor[:50])\n",
    "        means = rewards_tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.ones(49) * torch.mean(reward_f), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym environments custom wrapper to skip a specified number of frames.\n",
    "\n",
    "    Attributes:\n",
    "        env (gym.Env): The environment to wrap.\n",
    "        _skip (int): The number of frames to skip.\n",
    "\n",
    "    Methods:\n",
    "        step(action):\n",
    "            Repeats the given action for the specified number of frames and\n",
    "            accumulates the reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return state, total_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environment_continuous():\n",
    "    \"\"\"\n",
    "    Create a continuous version of the CarRacing-v3 environment with appropriate wrappers.\n",
    "    \"\"\"\n",
    "    env_cont = gym.make(\n",
    "        \"CarRacing-v3\",\n",
    "        render_mode=\"rgb_array\",\n",
    "        continuous=True\n",
    "    )\n",
    "    env_cont = SkipFrame(env_cont, skip=4)\n",
    "    env_cont = gym_wrap.GrayscaleObservation(env_cont)\n",
    "    env_cont = gym_wrap.ResizeObservation(env_cont, shape=(84, 84))\n",
    "    env_cont = gym_wrap.FrameStackObservation(env_cont, stack_size=4)\n",
    "    return env_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network (CNN) for feature extraction from high-dimensional input.\n",
    "\n",
    "    Attributes:\n",
    "        net (nn.Sequential): The sequential model defining the CNN architecture.\n",
    "\n",
    "    Methods:\n",
    "        __init__(input_dimensions, output_dimensions):\n",
    "            Initializes the CNN with the given input and output dimensions.\n",
    "        forward(input):\n",
    "            Defines the forward pass of the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimensions, output_dimensions):\n",
    "        super().__init__()\n",
    "        channel_n, height, width = input_dimensions\n",
    "\n",
    "        if height != 84 or width != 84:\n",
    "            raise ValueError(f\"Invalid input ({height, width})-shape. Expected: (84, 84)\")\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channel_n, out_channels=16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2592, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dimensions),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_CEM:\n",
    "    \"\"\"\n",
    "    Agent using the Cross-Entropy Method (CEM) for policy optimization.\n",
    "\n",
    "    Attributes:\n",
    "        model (torch.nn.Module): Neural network used as the policy.\n",
    "        mean (torch.Tensor): Flat vector with the current parameters (mean of the distribution).\n",
    "        sigma (float): Standard deviation for sampling.\n",
    "        population_size (int): Number of candidates per generation.\n",
    "        elite_frac (float): Fraction of the best candidates (elite).\n",
    "        num_elites (int): Number of elites (population_size * elite_frac).\n",
    "        device (torch.device): Device for computation.\n",
    "        dir_models (Path): Directory to save models.\n",
    "        dir_logs (Path): Directory to save logs.\n",
    "\n",
    "    Methods:\n",
    "        get_action(state): Returns the continuous action for a given state.\n",
    "        sample_candidate(): Generates a candidate (parameter vector) with noise.\n",
    "        update_policy(candidates, rewards): Updates the policy using the elite candidates.\n",
    "        save(save_name): Saves the current model and distribution parameters.\n",
    "        load(model_name): Loads saved model and parameters.\n",
    "        write_log(...): Records training metrics in a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_shape, action_dim, device, directory_models, directory_logs, CNN,\n",
    "                 population_size=50, elite_frac=0.2, initial_std=0.1, load_state=False, load_model=None):\n",
    "        self.device = device\n",
    "        self.dir_models = directory_models\n",
    "        self.dir_logs = directory_logs\n",
    "\n",
    "        self.population_size = population_size\n",
    "        self.elite_frac = elite_frac\n",
    "        self.num_elites = int(self.population_size * self.elite_frac)\n",
    "        self.sigma = initial_std\n",
    "\n",
    "        # Inicializa a rede de política (CNN)\n",
    "        self.model = CNN(state_shape, action_dim).float().to(self.device)\n",
    "\n",
    "        # Inicializa o vetor de parâmetros (média) a partir do modelo\n",
    "        self.mean = parameters_to_vector(self.model.parameters()).detach().clone()\n",
    "\n",
    "        if load_state:\n",
    "            if load_model is None:\n",
    "                raise ValueError(\"Especifique o nome do modelo para carregar.\")\n",
    "            self.load(load_model)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Retorna a ação contínua para o estado dado.\n",
    "\n",
    "        Para CarRacing-v3 contínuo, o espaço de ação é:\n",
    "         - steering: [-1, 1]\n",
    "         - gas: [0, 1]\n",
    "         - brake: [0, 1]\n",
    "\n",
    "        A rede gera três valores que são processados (tanh para steering, sigmoid para os demais).\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.model(state_tensor)\n",
    "        steering = torch.tanh(action[0, 0])\n",
    "        gas = torch.sigmoid(action[0, 1])\n",
    "        brake = torch.sigmoid(action[0, 2])\n",
    "        return np.array([steering.item(), gas.item(), brake.item()])\n",
    "\n",
    "    def sample_candidate(self):\n",
    "        \"\"\"\n",
    "        Gera um vetor candidato de parâmetros a partir da média atual, adicionando ruído gaussiano.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(self.mean) * self.sigma\n",
    "        candidate = self.mean + noise\n",
    "        return candidate, noise\n",
    "\n",
    "    def update_policy(self, candidates, rewards):\n",
    "        \"\"\"\n",
    "        Atualiza a política com base nos candidatos elite.\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        elite_indices = rewards.argsort()[-self.num_elites:]\n",
    "        elites = [candidates[i] for i in elite_indices]\n",
    "        new_mean = torch.stack(elites, dim=0).mean(dim=0)\n",
    "        # Atualiza sigma como o desvio padrão médio entre os elites\n",
    "        new_sigma = torch.stack(elites, dim=0).std(dim=0).mean().item()\n",
    "        self.mean = new_mean\n",
    "        self.sigma = new_sigma\n",
    "        # Atualiza os parâmetros do modelo\n",
    "        vector_to_parameters(self.mean, self.model.parameters())\n",
    "\n",
    "    def save(self, save_name='CEM'):\n",
    "        \"\"\"\n",
    "        Salva o modelo e os parâmetros da distribuição em arquivo.\n",
    "        \"\"\"\n",
    "        save_path = str(self.dir_models / f\"{save_name}.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'mean': self.mean,\n",
    "            'sigma': self.sigma\n",
    "        }, save_path)\n",
    "        print(f\"Modelo salvo em {save_path}\")\n",
    "\n",
    "    def load(self, model_name):\n",
    "        \"\"\"\n",
    "        Carrega o modelo e os parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        loaded = torch.load(str(self.dir_models / model_name))\n",
    "        self.model.load_state_dict(loaded['model_state_dict'])\n",
    "        self.mean = loaded['mean']\n",
    "        self.sigma = loaded['sigma']\n",
    "        vector_to_parameters(self.mean, self.model.parameters())\n",
    "        print(f\"Modelo {model_name} carregado.\")\n",
    "\n",
    "    def write_log(self, generations, best_rewards, avg_rewards, sigmas, log_filename='log_CEM.csv'):\n",
    "        \"\"\"\n",
    "        Escreve os logs de treinamento em um arquivo CSV.\n",
    "        \"\"\"\n",
    "        rows = [\n",
    "            ['generation'] + generations,\n",
    "            ['best_reward'] + best_rewards,\n",
    "            ['avg_reward'] + avg_rewards,\n",
    "            ['sigma'] + sigmas\n",
    "        ]\n",
    "        with open(str(self.dir_logs / log_filename), 'w') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env, episodes=1):\n",
    "    \"\"\"\n",
    "    Evaluate the current policy of the agent in the environment, returning the average reward.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# CEM hyperparameters\n",
    "POPULATION_SIZE = 10\n",
    "ELITE_FRAC = 0.2\n",
    "INITIAL_STD = 0.1\n",
    "GENERATIONS = 10        # Number of generations\n",
    "EVAL_EPISODES = 1      # Episodes to evaluate each candidate\n",
    "\n",
    "env = get_environment_continuous()\n",
    "state, info = env.reset()\n",
    "\n",
    "# For continuous control, the action dimension is 3\n",
    "action_dim = 3\n",
    "\n",
    "agent = Agent_CEM(\n",
    "    state_shape=state.shape,\n",
    "    action_dim=action_dim,\n",
    "    device=DEVICE,\n",
    "    directory_models=DIRECTORY_MODELS,\n",
    "    directory_logs=DIRECTORY_LOGS,\n",
    "    CNN=CNN,\n",
    "    population_size=POPULATION_SIZE,\n",
    "    elite_frac=ELITE_FRAC,\n",
    "    initial_std=INITIAL_STD,\n",
    "    load_state=False\n",
    ")\n",
    "\n",
    "# Lists for logging metrics\n",
    "generation_numbers = []\n",
    "best_rewards = []\n",
    "average_rewards = []\n",
    "sigma_values = []\n",
    "generation_dates = []\n",
    "generation_times = []\n",
    "\n",
    "interval_log = 5\n",
    "\n",
    "# Main training loop (by generation)\n",
    "for generation in range(1, GENERATIONS + 1):\n",
    "    candidates = []\n",
    "    rewards = []\n",
    "\n",
    "    # Generate the population and evaluate each candidate\n",
    "    for i in range(POPULATION_SIZE):\n",
    "        candidate_params, _ = agent.sample_candidate()\n",
    "        # Apply candidate parameters to the model\n",
    "        from torch.nn.utils import vector_to_parameters\n",
    "        vector_to_parameters(candidate_params, agent.model.parameters())\n",
    "        candidate_reward = evaluate_policy(agent, env, episodes=EVAL_EPISODES)\n",
    "        candidates.append(candidate_params)\n",
    "        rewards.append(candidate_reward)\n",
    "\n",
    "    best_reward = np.max(rewards)\n",
    "    avg_reward = np.mean(rewards)\n",
    "\n",
    "    # Update the policy with elite candidates\n",
    "    agent.update_policy(candidates, rewards)\n",
    "\n",
    "    # Record metrics for the current generation\n",
    "    generation_numbers.append(generation)\n",
    "    best_rewards.append(best_reward)\n",
    "    average_rewards.append(avg_reward)\n",
    "    sigma_values.append(agent.sigma)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    generation_dates.append(now.date().strftime('%Y-%m-%d'))\n",
    "    generation_times.append(now.time().strftime('%H:%M:%S'))\n",
    "\n",
    "    print(f\"Generation {generation}: Best Reward = {best_reward:.2f}, Average Reward = {avg_reward:.2f}, Sigma = {agent.sigma:.4f}\")\n",
    "\n",
    "    plot_reward(generation, best_rewards, sigma_values)\n",
    "\n",
    "    # Save the model every interval_log generations\n",
    "    if generation % interval_log == 0:\n",
    "        agent.save(save_name=f\"CEM_gen_{generation}\")\n",
    "        agent.write_log(\n",
    "            generation_numbers,\n",
    "            best_rewards,\n",
    "            average_rewards,\n",
    "            sigma_values\n",
    "        )\n",
    "\n",
    "# Final save and log writing\n",
    "agent.save()\n",
    "agent.write_log(\n",
    "    generation_numbers,\n",
    "    best_rewards,\n",
    "    average_rewards,\n",
    "    sigma_values\n",
    ")\n",
    "\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = 20  # Adjust based on the last trained model\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "# Directory where videos will be saved\n",
    "VIDEO_DIRNAME = \"cem_videos\"\n",
    "\n",
    "# Remove old video files\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    (DIRECTORY_FIGURES / VIDEO_DIRNAME / f\"cem-video-episode-{episode_index}.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "# Function to create the CarRacing environment with video recording\n",
    "def get_environment_continuous():\n",
    "    \"\"\"\n",
    "    Create a continuous version of the CarRacing-v3 environment with video recording.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=True)\n",
    "    env = gym_wrap.GrayscaleObservation(env)\n",
    "    env = gym_wrap.ResizeObservation(env, shape=(84, 84))\n",
    "    env = gym_wrap.FrameStackObservation(env, stack_size=4)\n",
    "    env = gym.wrappers.RecordVideo(env, video_folder=DIRECTORY_FIGURES / VIDEO_DIRNAME, episode_trigger=lambda x: True)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "    return env\n",
    "\n",
    "env = get_environment_continuous()\n",
    "state, info = env.reset()\n",
    "\n",
    "# For continuous control, the action dimension is 3\n",
    "action_dim = 3\n",
    "\n",
    "agent = Agent_CEM(\n",
    "    state_shape=state.shape,\n",
    "    action_dim=action_dim,\n",
    "    device=DEVICE,\n",
    "    directory_models=DIRECTORY_MODELS,\n",
    "    directory_logs=DIRECTORY_LOGS,\n",
    "    CNN=CNN,\n",
    "    load_state=\"eval\",\n",
    "    load_model=f\"CEM_gen_{MODEL_ID}.pt\"\n",
    ")\n",
    "agent.sigma = 0  # No exploration during evaluation\n",
    "\n",
    "# Run episodes and record videos\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    total_reward = 0.0\n",
    "    state, info = env.reset()\n",
    "    episode_over = False\n",
    "\n",
    "    while not episode_over:\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    print(f\"Episode {episode_index}, Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "# Print episode statistics\n",
    "print(f\"Episode total rewards: {env.return_queue}\")\n",
    "print(f\"Episode lengths: {env.length_queue}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(DIRECTORY_FIGURES / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(DIRECTORY_FIGURES / VIDEO_DIRNAME / \"rl-video-episode-1.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(DIRECTORY_FIGURES / VIDEO_DIRNAME / \"rl-video-episode-2.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(DIRECTORY_FIGURES / VIDEO_DIRNAME / \"rl-video-episode-3.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
