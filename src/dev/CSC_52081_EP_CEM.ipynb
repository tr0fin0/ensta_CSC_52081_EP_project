{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, List, Optional\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import Video\n",
    "from ipywidgets import interact\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\n",
      "output\\models\n",
      "output\\images\n",
      "output\\logs\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY_OUTPUT = \"output\"\n",
    "DIRECTORY_MODELS = Path(f\"{DIRECTORY_OUTPUT}/models/\")\n",
    "DIRECTORY_FIGURES = Path(f\"{DIRECTORY_OUTPUT}/images/\")\n",
    "DIRECTORY_LOGS = Path(f\"{DIRECTORY_OUTPUT}/logs/\")\n",
    "\n",
    "if not DIRECTORY_FIGURES.exists():\n",
    "    DIRECTORY_FIGURES.mkdir(parents=True)\n",
    "\n",
    "if not DIRECTORY_MODELS.exists():\n",
    "    DIRECTORY_MODELS.mkdir(parents=True)\n",
    "\n",
    "if not DIRECTORY_LOGS.exists():\n",
    "    DIRECTORY_LOGS.mkdir(parents=True)\n",
    "\n",
    "print(DIRECTORY_OUTPUT)\n",
    "print(DIRECTORY_MODELS)\n",
    "print(DIRECTORY_FIGURES)\n",
    "print(DIRECTORY_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs and model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(policy, iteration):\n",
    "    \"\"\"Save the model parameters.\"\"\"\n",
    "    model_path = DIRECTORY_MODELS / f\"model_iter_{iteration}.pt\"\n",
    "    torch.save(policy.state_dict(), model_path)\n",
    "    # print(f\"Model saved at iteration {iteration}: {model_path}\")\n",
    "\n",
    "def save_log(iteration, rewards, sigmas):\n",
    "    \"\"\"Save the logs in a CSV file.\"\"\"\n",
    "    log_path = DIRECTORY_LOGS / \"training_log.csv\"\n",
    "    data = {\"iteration\": iteration, \"reward\": rewards[-1], \"sigma\": sigmas[-1]}\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "    if log_path.exists():\n",
    "        df.to_csv(log_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(log_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    # print(f\"Log saved at iteration {iteration}: {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "def plot_reward(iteration, reward_list, sigmas):\n",
    "    \"\"\"\n",
    "    Plot the reward per generation and a moving average.\n",
    "\n",
    "    Args:\n",
    "        generation (int): Current generation number.\n",
    "        reward_list (list): List of best rewards per generation.\n",
    "        sigmas (list): List of sigma values per generation.\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    rewards_tensor = torch.tensor(reward_list, dtype=torch.float)\n",
    "\n",
    "    if len(rewards_tensor) >= 11:\n",
    "        eval_reward = torch.clone(rewards_tensor[-10:])\n",
    "        mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "        std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "        plt.clf()\n",
    "        plt.title(\n",
    "            f'Iter #{iteration}: Best Reward: {reward_list[-1]:.2f}, Sigma: {sigmas[-1]:.4f}, '\n",
    "            f'[{mean_eval_reward:.1f}Â±{std_eval_reward:.1f}]'\n",
    "        )\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_tensor.numpy())\n",
    "\n",
    "    if len(rewards_tensor) >= 50:\n",
    "        reward_f = torch.clone(rewards_tensor[:50])\n",
    "        means = rewards_tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.ones(49) * torch.mean(reward_f), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym environments custom wrapper to skip a specified number of frames.\n",
    "\n",
    "    Attributes:\n",
    "        env (gym.Env): The environment to wrap.\n",
    "        _skip (int): The number of frames to skip.\n",
    "\n",
    "    Methods:\n",
    "        step(action):\n",
    "            Repeats the given action for the specified number of frames and\n",
    "            accumulates the reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return state, total_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CNN for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network (CNN) for feature extraction from high-dimensional input.\n",
    "\n",
    "    The network expects input images with dimensions (channels, 84, 84) and outputs a feature vector\n",
    "    that is further processed to produce the action values.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimensions, output_dimensions):\n",
    "        \"\"\"\n",
    "        Initialize the CNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dimensions : tuple\n",
    "            A tuple (channels, height, width). For your setup, expected: (4, 84, 84)\n",
    "            (4 frames stacked, 84x84 each).\n",
    "        output_dimensions : int\n",
    "            The number of outputs (i.e. action dimensions).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        channel_n, height, width = input_dimensions\n",
    "        if height != 84 or width != 84:\n",
    "            raise ValueError(f\"Invalid input ({height}, {width})-shape. Expected: (84, 84)\")\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channel_n, out_channels=16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        conv_output_size = 2592\n",
    "\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dimensions)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv_net(input)\n",
    "        x = self.fc_net(x)\n",
    "\n",
    "        steering = torch.tanh(x[:, 0])\n",
    "        gas = torch.sigmoid(x[:, 1])    # Range [0, 1]\n",
    "        brake = torch.sigmoid(x[:, 2])  # Range [0, 1]\n",
    "\n",
    "        return torch.stack((steering, gas, brake), dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CNNPolicy that wraps the CNN and adds parameter management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network policy for CarRacing using a CNN to process image inputs.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(observation: np.ndarray) -> np.ndarray\n",
    "        Given an observation, compute the action vector.\n",
    "    get_params() -> np.ndarray\n",
    "        Return the network parameters as a flat numpy array.\n",
    "    set_params(params: np.ndarray) -> None\n",
    "        Set the network parameters from a flat numpy array.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimensions, action_size):\n",
    "        \"\"\"\n",
    "        Initialize the CNNPolicy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dimensions : tuple\n",
    "            The dimensions of the input image (channels, height, width), e.g. (4, 84, 84)\n",
    "            (4 frames stacked, 84x84 each).\n",
    "        action_size : int\n",
    "            The number of continuous actions (for CarRacing: 3).\n",
    "        \"\"\"\n",
    "        super(CNNPolicy, self).__init__()\n",
    "        self.cnn = CNN(input_dimensions, action_size).to(device)\n",
    "\n",
    "    def forward(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the action for a given observation.\n",
    "\n",
    "        The observation is expected to be in the shape (4, 84, 84) as produced by our\n",
    "        preprocessing pipeline (frame stacking, grayscale, resizing). If the observation\n",
    "        is provided as a tuple (as in Gymnasium v3), we extract the first element.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : np.ndarray\n",
    "            The input observation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The output action vector (steering, gas, braking).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # If observation is provided as a tuple, extract the observation.\n",
    "            if isinstance(observation, (tuple, list)):\n",
    "                observation = observation[0]\n",
    "            # The observation is already channel-first (4, 84, 84), so no transpose is needed.\n",
    "            observation_tensor = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "            # If the observation is not batched, add a batch dimension.\n",
    "            if observation_tensor.ndim == 3:\n",
    "                observation_tensor = observation_tensor.unsqueeze(0)\n",
    "            output_tensor = self.cnn(observation_tensor)\n",
    "            action = output_tensor.squeeze().detach().cpu().numpy()\n",
    "\n",
    "            # # Optionally threshold gas and brake outputs.\n",
    "            # action[1] = 1 if action[1] > 0.5 else 0  # Gas (index 1)\n",
    "            # action[2] = 1 if action[2] > 0.5 else 0  # Brake (index 2)\n",
    "            return action\n",
    "\n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get a flat numpy array of the network parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The flattened parameters.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.nn.utils.parameters_to_vector(self.parameters())\n",
    "        return params_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def set_params(self, params: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the network parameters from a flat numpy array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : np.ndarray\n",
    "            The new parameters.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, device=device)\n",
    "        torch.nn.utils.vector_to_parameters(params_tensor, self.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Objective Function for evaluating the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "    \"\"\"\n",
    "    Objective function for evaluating a policy in the given environment.\n",
    "\n",
    "    This function runs the policy for a number of episodes and returns the negative of\n",
    "    the average reward (to transform the problem into a minimization task).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment (CarRacing-v3) in which to evaluate the policy.\n",
    "    policy : nn.Module\n",
    "        The policy to evaluate.\n",
    "    num_episodes : int, optional\n",
    "        Number of episodes per evaluation.\n",
    "    max_time_steps : float, optional\n",
    "        Maximum time steps per episode.\n",
    "    minimization_solver : bool, optional\n",
    "        If True, returns negative reward for minimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, policy: nn.Module, num_episodes: int = 1,\n",
    "                 max_time_steps: float = float(\"inf\"), minimization_solver: bool = True):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "        self.num_evals = 0\n",
    "\n",
    "    def eval(self, policy_params: np.ndarray, num_episodes: Optional[int] = None,\n",
    "             max_time_steps: Optional[float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the policy with the given parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy_params : np.ndarray\n",
    "            The parameters to evaluate.\n",
    "        num_episodes : int, optional\n",
    "            Number of episodes for evaluation.\n",
    "        max_time_steps : float, optional\n",
    "            Maximum time steps per episode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The (possibly negated) average total reward.\n",
    "        \"\"\"\n",
    "        self.policy.set_params(policy_params)\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        total_reward_sum = 0.0\n",
    "        for _ in range(num_episodes):\n",
    "            # Unpack the tuple returned by env.reset()\n",
    "            observation, info = self.env.reset()\n",
    "            episode_reward = 0.0\n",
    "            for t in range(int(max_time_steps)):\n",
    "                action = self.policy(observation)\n",
    "                # Unpack the tuple returned by env.step()\n",
    "                observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            total_reward_sum += episode_reward\n",
    "        avg_reward = total_reward_sum / num_episodes\n",
    "        # Convert to a minimization problem if required.\n",
    "        if self.minimization_solver:\n",
    "            avg_reward *= -1.0\n",
    "        return avg_reward\n",
    "\n",
    "    def __call__(self, policy_params: np.ndarray, num_episodes: Optional[int] = None,\n",
    "                 max_time_steps: Optional[float] = None) -> float:\n",
    "        return self.eval(policy_params, num_episodes, max_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Cross-Entropy Method (CEM) optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_uncorrelated(\n",
    "    objective_function: Callable[[np.ndarray], float],\n",
    "    mean_array: np.ndarray,\n",
    "    var_array: np.ndarray,\n",
    "    max_iterations: int = 500,\n",
    "    sample_size: int = 50,\n",
    "    elite_frac: float = 0.2,\n",
    "    print_every: int = 10,\n",
    "    success_score: float = float(\"inf\"),\n",
    "    num_evals_for_stop: Optional[int] = None,\n",
    "    hist_dict: Optional[dict] = None,\n",
    "    policy: Optional[nn.Module] = None,\n",
    "    save_interval: int = 50,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Method (CEM) optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    objective_function : Callable[[np.ndarray], float]\n",
    "        The objective function to evaluate policies.\n",
    "    mean_array : np.ndarray\n",
    "        Initial mean parameters.\n",
    "    var_array : np.ndarray\n",
    "        Initial variance for each parameter.\n",
    "    max_iterations : int, optional\n",
    "        Maximum number of iterations.\n",
    "    sample_size : int, optional\n",
    "        Number of candidate samples per iteration.\n",
    "    elite_frac : float, optional\n",
    "        Fraction of top-performing samples to use for updating.\n",
    "    print_every : int, optional\n",
    "        Frequency of printing and plotting progress.\n",
    "    success_score : float, optional\n",
    "        Score threshold for early stopping.\n",
    "    num_evals_for_stop : Optional[int], optional\n",
    "        If provided, evaluate the mean parameters every iteration.\n",
    "    hist_dict : Optional[dict], optional\n",
    "        Dictionary to store history.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The optimized mean parameters.\n",
    "    \"\"\"\n",
    "    assert 0.0 < elite_frac <= 1.0\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    # Lists for real-time plotting\n",
    "    reward_list = []\n",
    "    sigma_list = []\n",
    "\n",
    "    for iteration_index in range(max_iterations):\n",
    "        # Sample new candidate solutions from the multivariate normal distribution.\n",
    "        x_array = np.random.randn(sample_size, mean_array.shape[0]) * np.sqrt(var_array) + mean_array\n",
    "\n",
    "        # Evaluate each candidate solution.\n",
    "        score_array = np.array([objective_function(x) for x in x_array])\n",
    "        sorted_indices = np.argsort(score_array)\n",
    "        elite_indices = sorted_indices[:n_elite]\n",
    "        elite_x_array = x_array[elite_indices]\n",
    "\n",
    "        # Update mean and variance based on the elite samples.\n",
    "        mean_array = np.mean(elite_x_array, axis=0)\n",
    "        var_array = np.var(elite_x_array, axis=0)\n",
    "        score = np.mean(score_array[elite_indices])\n",
    "\n",
    "        # Append values for plotting.\n",
    "        reward_list.append(score)\n",
    "        sigma_list.append(np.mean(var_array))\n",
    "\n",
    "        if iteration_index % print_every == 0:\n",
    "            print(f\"Iteration {iteration_index}\\tScore {score}\")\n",
    "            plot_reward(iteration_index, reward_list, sigma_list)\n",
    "\n",
    "        if iteration_index % save_interval == 0:\n",
    "            save_model(policy, iteration_index)\n",
    "\n",
    "        save_log(iteration_index, reward_list, sigma_list)\n",
    "\n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [score] + mean_array.tolist() + var_array.tolist()\n",
    "\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = objective_function(mean_array)\n",
    "        if score <= success_score:\n",
    "            break\n",
    "\n",
    "    return mean_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CEM agent on the CarRacing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m init_var_array = np.ones(num_params) * \u001b[32m1000.0\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Optimize the policy parameters using CEM.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m optimized_policy_params = \u001b[43mcem_uncorrelated\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmean_array\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_mean_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvar_array\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_var_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43melite_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuccess_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m700\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhist_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhist_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcnn_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# # Optionally, update the policy with the optimized parameters.\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# cnn_policy.set_params(optimized_policy_params)\u001b[39;00m\n\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Close the environment.\u001b[39;00m\n\u001b[32m     48\u001b[39m env.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mcem_uncorrelated\u001b[39m\u001b[34m(objective_function, mean_array, var_array, max_iterations, sample_size, elite_frac, print_every, success_score, num_evals_for_stop, hist_dict, policy, save_interval)\u001b[39m\n\u001b[32m     55\u001b[39m x_array = np.random.randn(sample_size, mean_array.shape[\u001b[32m0\u001b[39m]) * np.sqrt(var_array) + mean_array\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Evaluate each candidate solution.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m score_array = np.array([\u001b[43mobjective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_array])\n\u001b[32m     59\u001b[39m sorted_indices = np.argsort(score_array)\n\u001b[32m     60\u001b[39m elite_indices = sorted_indices[:n_elite]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mObjectiveFunction.__call__\u001b[39m\u001b[34m(self, policy_params, num_episodes, max_time_steps)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_params: np.ndarray, num_episodes: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     77\u001b[39m              max_time_steps: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_time_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mObjectiveFunction.eval\u001b[39m\u001b[34m(self, policy_params, num_episodes, max_time_steps)\u001b[39m\n\u001b[32m     63\u001b[39m action = \u001b[38;5;28mself\u001b[39m.policy(observation)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Unpack the tuple returned by env.step()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m episode_reward += reward\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:425\u001b[39m, in \u001b[36mFrameStackObservation.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    415\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    416\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    417\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m    419\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    423\u001b[39m \u001b[33;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.obs_queue.append(obs)\n\u001b[32m    428\u001b[39m     updated_obs = deepcopy(\n\u001b[32m    429\u001b[39m         concatenate(\u001b[38;5;28mself\u001b[39m.env.observation_space, \u001b[38;5;28mself\u001b[39m.obs_queue, \u001b[38;5;28mself\u001b[39m.stacked_obs)\n\u001b[32m    430\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\core.py:560\u001b[39m, in \u001b[36mObservationWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    558\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    559\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observation(observation), reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\core.py:560\u001b[39m, in \u001b[36mObservationWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    558\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    559\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observation(observation), reward, terminated, truncated, info\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mSkipFrame.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     19\u001b[39m total_reward = \u001b[32m0.0\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._skip):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     state, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     total_reward += reward\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:563\u001b[39m, in \u001b[36mCarRacing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.world.Step(\u001b[32m1.0\u001b[39m / FPS, \u001b[32m6\u001b[39m * \u001b[32m30\u001b[39m, \u001b[32m2\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m    561\u001b[39m \u001b[38;5;28mself\u001b[39m.t += \u001b[32m1.0\u001b[39m / FPS\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;28mself\u001b[39m.state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_pixels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m step_reward = \u001b[32m0\u001b[39m\n\u001b[32m    566\u001b[39m terminated = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:628\u001b[39m, in \u001b[36mCarRacing._render\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m    625\u001b[39m trans = pygame.math.Vector2((scroll_x, scroll_y)).rotate_rad(angle)\n\u001b[32m    626\u001b[39m trans = (WINDOW_W / \u001b[32m2\u001b[39m + trans[\u001b[32m0\u001b[39m], WINDOW_H / \u001b[32m4\u001b[39m + trans[\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_render_road\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[38;5;28mself\u001b[39m.car.draw(\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m.surf,\n\u001b[32m    631\u001b[39m     zoom,\n\u001b[32m   (...)\u001b[39m\u001b[32m    634\u001b[39m     mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mstate_pixels_list\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstate_pixels\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    635\u001b[39m )\n\u001b[32m    637\u001b[39m \u001b[38;5;28mself\u001b[39m.surf = pygame.transform.flip(\u001b[38;5;28mself\u001b[39m.surf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:672\u001b[39m, in \u001b[36mCarRacing._render_road\u001b[39m\u001b[34m(self, zoom, translation, angle)\u001b[39m\n\u001b[32m    664\u001b[39m field = [\n\u001b[32m    665\u001b[39m     (bounds, bounds),\n\u001b[32m    666\u001b[39m     (bounds, -bounds),\n\u001b[32m    667\u001b[39m     (-bounds, -bounds),\n\u001b[32m    668\u001b[39m     (-bounds, bounds),\n\u001b[32m    669\u001b[39m ]\n\u001b[32m    671\u001b[39m \u001b[38;5;66;03m# draw background\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_draw_colored_polygon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbg_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    674\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;66;03m# draw grass patches\u001b[39;00m\n\u001b[32m    677\u001b[39m grass = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mateus\\OneDrive\\Documentos\\7.ENSTA\\3A\\RI\\CSC_52081-AdvancedIA\\Project\\ensta_CSC_52081_EP_project\\env-project\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:785\u001b[39m, in \u001b[36mCarRacing._draw_colored_polygon\u001b[39m\u001b[34m(self, surface, poly, color, zoom, translation, angle, clip)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# This checks if the polygon is out of bounds of the screen, and we skip drawing if so.\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[38;5;66;03m# Instead of calculating exactly if the polygon and screen overlap,\u001b[39;00m\n\u001b[32m    777\u001b[39m \u001b[38;5;66;03m# we simply check if the polygon is in a larger bounding box whose dimension\u001b[39;00m\n\u001b[32m    778\u001b[39m \u001b[38;5;66;03m# is greater than the screen by MAX_SHAPE_DIM, which is the maximum\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[38;5;66;03m# diagonal length of an environment object\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m clip \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    781\u001b[39m     (-MAX_SHAPE_DIM <= coord[\u001b[32m0\u001b[39m] <= WINDOW_W + MAX_SHAPE_DIM)\n\u001b[32m    782\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (-MAX_SHAPE_DIM <= coord[\u001b[32m1\u001b[39m] <= WINDOW_H + MAX_SHAPE_DIM)\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m poly\n\u001b[32m    784\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m     \u001b[43mgfxdraw\u001b[49m\u001b[43m.\u001b[49m\u001b[43maapolygon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m     gfxdraw.filled_polygon(\u001b[38;5;28mself\u001b[39m.surf, poly, color)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create the CarRacing-v3 environment (continuous mode)\n",
    "    env = gym.make(\"CarRacing-v3\",\n",
    "                render_mode=\"rgb_array\",\n",
    "                lap_complete_percent=0.95,\n",
    "                continuous=True)\n",
    "\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = gym_wrap.GrayscaleObservation(env)\n",
    "    env = gym_wrap.ResizeObservation(env, shape=(84, 84))\n",
    "    env = gym_wrap.FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "    # The environment provides 84x84 RGB images.\n",
    "    input_dimensions = (4, 84, 84)  # 4 frames, 84x84 each.\n",
    "    action_size = 3  # [steering, gas, braking]\n",
    "\n",
    "    cnn_policy = CNNPolicy(input_dimensions, action_size)\n",
    "\n",
    "    # Set up the objective function.\n",
    "    objective_function = ObjectiveFunction(env, cnn_policy, num_episodes=1, max_time_steps=100)\n",
    "\n",
    "    # Prepare the initial distribution for CEM.\n",
    "    hist_dict = {}\n",
    "    num_params = len(cnn_policy.get_params())\n",
    "    init_mean_array = np.random.random(num_params)\n",
    "    init_var_array = np.ones(num_params) * 1000.0\n",
    "\n",
    "    # Optimize the policy parameters using CEM.\n",
    "    optimized_policy_params = cem_uncorrelated(\n",
    "        objective_function=objective_function,\n",
    "        mean_array=init_mean_array,\n",
    "        var_array=init_var_array,\n",
    "        max_iterations=200,\n",
    "        sample_size=10,\n",
    "        elite_frac=0.3,\n",
    "        print_every=1,\n",
    "        success_score=-700,\n",
    "        hist_dict=hist_dict,\n",
    "        policy=cnn_policy,\n",
    "        save_interval=10\n",
    "    )\n",
    "\n",
    "    # # Optionally, update the policy with the optimized parameters.\n",
    "    # cnn_policy.set_params(optimized_policy_params)\n",
    "\n",
    "    # Close the environment.\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_selector(file_path: List[Path]) -> Video:\n",
    "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select the episode to play here ğ\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a827ee983ee425cbbf44d87a850854d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='file_path', options=(WindowsPath('output/images/cem_carracing_traiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a video prefix and number of episodes to record.\n",
    "VIDEO_PREFIX_CEM_TRAINED = \"cem_carracing_trained\"\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "# Create a list of file paths for the videos.\n",
    "file_path_list = [\n",
    "    DIRECTORY_FIGURES / f\"{VIDEO_PREFIX_CEM_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "# Remove any existing videos with the same names.\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "# Create the CarRacing-v3 environment with rendering as RGB frames.\n",
    "env = gym.make(\"CarRacing-v3\",\n",
    "                render_mode=\"rgb_array\",\n",
    "                lap_complete_percent=0.95,\n",
    "                continuous=True)\n",
    "\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = gym_wrap.GrayscaleObservation(env)\n",
    "env = gym_wrap.ResizeObservation(env, shape=(84, 84))\n",
    "env = gym_wrap.FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "# Wrap the environment to record videos of every episode.\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(DIRECTORY_FIGURES),\n",
    "    name_prefix=VIDEO_PREFIX_CEM_TRAINED,\n",
    "    episode_trigger=lambda episode: True,  # Record every episode.\n",
    ")\n",
    "# Optionally, wrap to record episode statistics.\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "# Create an ObjectiveFunction instance using the environment and your trained policy.\n",
    "# Here, 'cnn_policy' should be your CNNPolicy instance and 'optimized_policy_params'\n",
    "# should be the parameters you obtained from training.\n",
    "objective_function = ObjectiveFunction(env=env, policy=cnn_policy)\n",
    "\n",
    "# Evaluate the policy on NUM_EPISODES episodes, with a maximum of 1000 time steps per episode.\n",
    "objective_function.eval(optimized_policy_params, num_episodes=NUM_EPISODES, max_time_steps=1000)\n",
    "\n",
    "# Close the environment after evaluation.\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ğ\\n\")\n",
    "\n",
    "# 'video_selector' should be a function that takes a file path and displays the video.\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
