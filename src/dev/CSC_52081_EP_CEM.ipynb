{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, List, Optional\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import Video\n",
    "from ipywidgets import interact\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\n",
      "output\\models\n",
      "output\\images\n",
      "output\\logs\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY_OUTPUT = \"output\"\n",
    "DIRECTORY_MODELS = Path(f\"{DIRECTORY_OUTPUT}/models/\")\n",
    "DIRECTORY_FIGURES = Path(f\"{DIRECTORY_OUTPUT}/images/\")\n",
    "DIRECTORY_LOGS = Path(f\"{DIRECTORY_OUTPUT}/logs/\")\n",
    "\n",
    "if not DIRECTORY_FIGURES.exists():\n",
    "    DIRECTORY_FIGURES.mkdir(parents=True)\n",
    "\n",
    "if not DIRECTORY_MODELS.exists():\n",
    "    DIRECTORY_MODELS.mkdir(parents=True)\n",
    "\n",
    "if not DIRECTORY_LOGS.exists():\n",
    "    DIRECTORY_LOGS.mkdir(parents=True)\n",
    "\n",
    "print(DIRECTORY_OUTPUT)\n",
    "print(DIRECTORY_MODELS)\n",
    "print(DIRECTORY_FIGURES)\n",
    "print(DIRECTORY_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs and model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(policy, iteration):\n",
    "    \"\"\"Save the model parameters.\"\"\"\n",
    "    model_path = DIRECTORY_MODELS / f\"model_iter_{iteration}.pt\"\n",
    "    torch.save(policy.state_dict(), model_path)\n",
    "    # print(f\"Model saved at iteration {iteration}: {model_path}\")\n",
    "\n",
    "def save_log(iteration, rewards, sigmas):\n",
    "    \"\"\"Save the logs in a CSV file.\"\"\"\n",
    "    log_path = DIRECTORY_LOGS / \"training_log.csv\"\n",
    "    data = {\"iteration\": iteration, \"reward\": rewards[-1], \"sigma\": sigmas[-1]}\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "    if log_path.exists():\n",
    "        df.to_csv(log_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(log_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    # print(f\"Log saved at iteration {iteration}: {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "def plot_reward(iteration, reward_list, sigmas):\n",
    "    \"\"\"\n",
    "    Plot the reward per generation and a moving average.\n",
    "\n",
    "    Args:\n",
    "        generation (int): Current generation number.\n",
    "        reward_list (list): List of best rewards per generation.\n",
    "        sigmas (list): List of sigma values per generation.\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    rewards_tensor = torch.tensor(reward_list, dtype=torch.float)\n",
    "\n",
    "    if len(rewards_tensor) >= 11:\n",
    "        eval_reward = torch.clone(rewards_tensor[-10:])\n",
    "        mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
    "        std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
    "        plt.clf()\n",
    "        plt.title(\n",
    "            f'Iter #{iteration}: Best Reward: {reward_list[-1]:.2f}, Sigma: {sigmas[-1]:.4f}, '\n",
    "            f'[{mean_eval_reward:.1f}Â±{std_eval_reward:.1f}]'\n",
    "        )\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_tensor.numpy())\n",
    "\n",
    "    if len(rewards_tensor) >= 50:\n",
    "        reward_f = torch.clone(rewards_tensor[:50])\n",
    "        means = rewards_tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.ones(49) * torch.mean(reward_f), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym environments custom wrapper to skip a specified number of frames.\n",
    "\n",
    "    Attributes:\n",
    "        env (gym.Env): The environment to wrap.\n",
    "        _skip (int): The number of frames to skip.\n",
    "\n",
    "    Methods:\n",
    "        step(action):\n",
    "            Repeats the given action for the specified number of frames and\n",
    "            accumulates the reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return state, total_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CNN for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network (CNN) for feature extraction from high-dimensional input.\n",
    "\n",
    "    The network expects input images with dimensions (channels, 84, 84) and outputs a feature vector\n",
    "    that is further processed to produce the action values.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimensions, output_dimensions):\n",
    "        \"\"\"\n",
    "        Initialize the CNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dimensions : tuple\n",
    "            A tuple (channels, height, width). For your setup, expected: (4, 84, 84)\n",
    "            (4 frames stacked, 84x84 each).\n",
    "        output_dimensions : int\n",
    "            The number of outputs (i.e. action dimensions).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        channel_n, height, width = input_dimensions\n",
    "        if height != 84 or width != 84:\n",
    "            raise ValueError(f\"Invalid input ({height}, {width})-shape. Expected: (84, 84)\")\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channel_n, out_channels=16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        conv_output_size = 2592\n",
    "\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dimensions)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv_net(input)\n",
    "        x = self.fc_net(x)\n",
    "\n",
    "        steering = torch.tanh(x[:, 0])\n",
    "        gas = torch.sigmoid(x[:, 1])    # Range [0, 1]\n",
    "        brake = torch.sigmoid(x[:, 2])  # Range [0, 1]\n",
    "\n",
    "        return torch.stack((steering, gas, brake), dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CNNPolicy that wraps the CNN and adds parameter management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network policy for CarRacing using a CNN to process image inputs.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(observation: np.ndarray) -> np.ndarray\n",
    "        Given an observation, compute the action vector.\n",
    "    get_params() -> np.ndarray\n",
    "        Return the network parameters as a flat numpy array.\n",
    "    set_params(params: np.ndarray) -> None\n",
    "        Set the network parameters from a flat numpy array.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimensions, action_size):\n",
    "        \"\"\"\n",
    "        Initialize the CNNPolicy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dimensions : tuple\n",
    "            The dimensions of the input image (channels, height, width), e.g. (4, 84, 84)\n",
    "            (4 frames stacked, 84x84 each).\n",
    "        action_size : int\n",
    "            The number of continuous actions (for CarRacing: 3).\n",
    "        \"\"\"\n",
    "        super(CNNPolicy, self).__init__()\n",
    "        self.cnn = CNN(input_dimensions, action_size).to(device)\n",
    "\n",
    "    def forward(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the action for a given observation.\n",
    "\n",
    "        The observation is expected to be in the shape (4, 84, 84) as produced by our\n",
    "        preprocessing pipeline (frame stacking, grayscale, resizing). If the observation\n",
    "        is provided as a tuple (as in Gymnasium v3), we extract the first element.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : np.ndarray\n",
    "            The input observation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The output action vector (steering, gas, braking).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # If observation is provided as a tuple, extract the observation.\n",
    "            if isinstance(observation, (tuple, list)):\n",
    "                observation = observation[0]\n",
    "            # The observation is already channel-first (4, 84, 84), so no transpose is needed.\n",
    "            observation_tensor = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "            # If the observation is not batched, add a batch dimension.\n",
    "            if observation_tensor.ndim == 3:\n",
    "                observation_tensor = observation_tensor.unsqueeze(0)\n",
    "            output_tensor = self.cnn(observation_tensor)\n",
    "            action = output_tensor.squeeze().detach().cpu().numpy()\n",
    "\n",
    "            # # Optionally threshold gas and brake outputs.\n",
    "            # action[1] = 1 if action[1] > 0.5 else 0  # Gas (index 1)\n",
    "            # action[2] = 1 if action[2] > 0.5 else 0  # Brake (index 2)\n",
    "            return action\n",
    "\n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get a flat numpy array of the network parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The flattened parameters.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.nn.utils.parameters_to_vector(self.parameters())\n",
    "        return params_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def set_params(self, params: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the network parameters from a flat numpy array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : np.ndarray\n",
    "            The new parameters.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, device=device)\n",
    "        torch.nn.utils.vector_to_parameters(params_tensor, self.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Objective Function for evaluating the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "    \"\"\"\n",
    "    Objective function for evaluating a policy in the given environment.\n",
    "\n",
    "    This function runs the policy for a number of episodes and returns the negative of\n",
    "    the average reward (to transform the problem into a minimization task).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment (CarRacing-v3) in which to evaluate the policy.\n",
    "    policy : nn.Module\n",
    "        The policy to evaluate.\n",
    "    num_episodes : int, optional\n",
    "        Number of episodes per evaluation.\n",
    "    max_time_steps : float, optional\n",
    "        Maximum time steps per episode.\n",
    "    minimization_solver : bool, optional\n",
    "        If True, returns negative reward for minimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, policy: nn.Module, num_episodes: int = 1,\n",
    "                 max_time_steps: float = float(\"inf\"), minimization_solver: bool = True):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "        self.num_evals = 0\n",
    "\n",
    "    def eval(self, policy_params: np.ndarray, num_episodes: Optional[int] = None,\n",
    "             max_time_steps: Optional[float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the policy with the given parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy_params : np.ndarray\n",
    "            The parameters to evaluate.\n",
    "        num_episodes : int, optional\n",
    "            Number of episodes for evaluation.\n",
    "        max_time_steps : float, optional\n",
    "            Maximum time steps per episode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The (possibly negated) average total reward.\n",
    "        \"\"\"\n",
    "        self.policy.set_params(policy_params)\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        total_reward_sum = 0.0\n",
    "        for _ in range(num_episodes):\n",
    "            # Unpack the tuple returned by env.reset()\n",
    "            observation, info = self.env.reset()\n",
    "            episode_reward = 0.0\n",
    "            for t in range(int(max_time_steps)):\n",
    "                action = self.policy(observation)\n",
    "                # Unpack the tuple returned by env.step()\n",
    "                observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            total_reward_sum += episode_reward\n",
    "        avg_reward = total_reward_sum / num_episodes\n",
    "        # Convert to a minimization problem if required.\n",
    "        if self.minimization_solver:\n",
    "            avg_reward *= -1.0\n",
    "        return avg_reward\n",
    "\n",
    "    def __call__(self, policy_params: np.ndarray, num_episodes: Optional[int] = None,\n",
    "                 max_time_steps: Optional[float] = None) -> float:\n",
    "        return self.eval(policy_params, num_episodes, max_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Cross-Entropy Method (CEM) optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_uncorrelated(\n",
    "    objective_function: Callable[[np.ndarray], float],\n",
    "    mean_array: np.ndarray,\n",
    "    var_array: np.ndarray,\n",
    "    max_iterations: int = 500,\n",
    "    sample_size: int = 50,\n",
    "    elite_frac: float = 0.2,\n",
    "    print_every: int = 10,\n",
    "    success_score: float = float(\"inf\"),\n",
    "    num_evals_for_stop: Optional[int] = None,\n",
    "    hist_dict: Optional[dict] = None,\n",
    "    policy: Optional[nn.Module] = None,\n",
    "    save_interval: int = 50\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Method (CEM) optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    objective_function : Callable[[np.ndarray], float]\n",
    "        The objective function to evaluate policies.\n",
    "    mean_array : np.ndarray\n",
    "        Initial mean parameters.\n",
    "    var_array : np.ndarray\n",
    "        Initial variance for each parameter.\n",
    "    max_iterations : int, optional\n",
    "        Maximum number of iterations.\n",
    "    sample_size : int, optional\n",
    "        Number of candidate samples per iteration.\n",
    "    elite_frac : float, optional\n",
    "        Fraction of top-performing samples to use for updating.\n",
    "    print_every : int, optional\n",
    "        Frequency of printing and plotting progress.\n",
    "    success_score : float, optional\n",
    "        Score threshold for early stopping.\n",
    "    num_evals_for_stop : Optional[int], optional\n",
    "        If provided, evaluate the mean parameters every iteration.\n",
    "    hist_dict : Optional[dict], optional\n",
    "        Dictionary to store history.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The optimized mean parameters.\n",
    "    \"\"\"\n",
    "    assert 0.0 < elite_frac <= 1.0\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    # Lists for real-time plotting\n",
    "    reward_list = []\n",
    "    sigma_list = []\n",
    "\n",
    "    for iteration_index in range(max_iterations):\n",
    "        # Sample new candidate solutions from the multivariate normal distribution.\n",
    "        x_array = np.random.randn(sample_size, mean_array.shape[0]) * np.sqrt(var_array) + mean_array\n",
    "\n",
    "        # Evaluate each candidate solution.\n",
    "        score_array = np.array([objective_function(x) for x in x_array])\n",
    "        sorted_indices = np.argsort(score_array)\n",
    "        elite_indices = sorted_indices[:n_elite]\n",
    "        elite_x_array = x_array[elite_indices]\n",
    "\n",
    "        # Update mean and variance based on the elite samples.\n",
    "        mean_array = np.mean(elite_x_array, axis=0)\n",
    "        var_array = np.var(elite_x_array, axis=0)\n",
    "        score = np.mean(score_array[elite_indices])\n",
    "\n",
    "        # Append values for plotting.\n",
    "        reward_list.append(score)\n",
    "        sigma_list.append(np.mean(var_array))\n",
    "\n",
    "        if iteration_index % print_every == 0:\n",
    "            print(f\"Iteration {iteration_index}\\tScore {score}\")\n",
    "            plot_reward(iteration_index, reward_list, sigma_list)\n",
    "\n",
    "        if iteration_index % save_interval == 0:\n",
    "            save_model(policy, iteration_index)\n",
    "\n",
    "        save_log(iteration_index, reward_list, sigma_list)\n",
    "\n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [score] + mean_array.tolist() + var_array.tolist()\n",
    "\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = objective_function(mean_array)\n",
    "        if score <= success_score:\n",
    "            break\n",
    "\n",
    "    return mean_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CEM agent on the CarRacing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create the CarRacing-v3 environment (continuous mode)\n",
    "    env = gym.make(\"CarRacing-v3\",\n",
    "                render_mode=\"rgb_array\",\n",
    "                lap_complete_percent=0.95,\n",
    "                continuous=True)\n",
    "\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = gym_wrap.GrayscaleObservation(env)\n",
    "    env = gym_wrap.ResizeObservation(env, shape=(84, 84))\n",
    "    env = gym_wrap.FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "    # The environment provides 84x84 RGB images.\n",
    "    input_dimensions = (4, 84, 84)  # 4 frames, 84x84 each.\n",
    "    action_size = 3  # [steering, gas, braking]\n",
    "\n",
    "    cnn_policy = CNNPolicy(input_dimensions, action_size)\n",
    "\n",
    "    # Set up the objective function.\n",
    "    objective_function = ObjectiveFunction(env, cnn_policy, num_episodes=1, max_time_steps=100)\n",
    "\n",
    "    # Prepare the initial distribution for CEM.\n",
    "    hist_dict = {}\n",
    "    num_params = len(cnn_policy.get_params())\n",
    "    init_mean_array = np.random.random(num_params)\n",
    "    init_var_array = np.ones(num_params) * 1000.0\n",
    "\n",
    "    # Optimize the policy parameters using CEM.\n",
    "    optimized_policy_params = cem_uncorrelated(\n",
    "        objective_function=objective_function,\n",
    "        mean_array=init_mean_array,\n",
    "        var_array=init_var_array,\n",
    "        max_iterations=200,\n",
    "        sample_size=10,\n",
    "        elite_frac=0.3,\n",
    "        print_every=1,\n",
    "        success_score=-700,\n",
    "        hist_dict=hist_dict,\n",
    "        policy=cnn_policy,\n",
    "        save_interval=10\n",
    "    )\n",
    "\n",
    "    # # Optionally, update the policy with the optimized parameters.\n",
    "    # cnn_policy.set_params(optimized_policy_params)\n",
    "\n",
    "    # Close the environment.\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_selector(file_path: List[Path]) -> Video:\n",
    "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select the episode to play here ðŸ‘‡\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76219a2396fe453e998edac01f732bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='file_path', options=(WindowsPath('output/images/cem_carracing_traiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a video prefix and number of episodes to record.\n",
    "VIDEO_PREFIX_CEM_TRAINED = \"cem_carracing_trained\"\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "# Create a list of file paths for the videos.\n",
    "file_path_list = [\n",
    "    DIRECTORY_FIGURES / f\"{VIDEO_PREFIX_CEM_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "# Remove any existing videos with the same names.\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "# Create the CarRacing-v3 environment with rendering as RGB frames.\n",
    "env = gym.make(\"CarRacing-v3\",\n",
    "                render_mode=\"rgb_array\",\n",
    "                lap_complete_percent=0.95,\n",
    "                continuous=True)\n",
    "\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = gym_wrap.GrayscaleObservation(env)\n",
    "env = gym_wrap.ResizeObservation(env, shape=(84, 84))\n",
    "env = gym_wrap.FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "# Wrap the environment to record videos of every episode.\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(DIRECTORY_FIGURES),\n",
    "    name_prefix=VIDEO_PREFIX_CEM_TRAINED,\n",
    "    episode_trigger=lambda episode: True,  # Record every episode.\n",
    ")\n",
    "# Optionally, wrap to record episode statistics.\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "# Create an ObjectiveFunction instance using the environment and your trained policy.\n",
    "# Here, 'cnn_policy' should be your CNNPolicy instance and 'optimized_policy_params'\n",
    "# should be the parameters you obtained from training.\n",
    "objective_function = ObjectiveFunction(env=env, policy=cnn_policy)\n",
    "\n",
    "# Evaluate the policy on NUM_EPISODES episodes, with a maximum of 1000 time steps per episode.\n",
    "objective_function.eval(optimized_policy_params, num_episodes=NUM_EPISODES, max_time_steps=1000)\n",
    "\n",
    "# Close the environment after evaluation.\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "# 'video_selector' should be a function that takes a file path and displays the video.\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
