{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC_52081_EP Project\n",
    "\n",
    "Advanced Machine Learning and Autonomous Agents Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) has emerged as a robust framework for training autonomous agents to learn optimal behaviors through environmental interactions. This study utilizes the [`CarRacing-v3`](https://gymnasium.farama.org/environments/box2d/car_racing/) environment from Gymnasium, which presents a challenging control task in a racing scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment features a high-dimensional observation space, represented by a $96 \\times 96$ RGB image capturing the car and track, necessitating the use of deep convolutional neural networks (CNNs) for effective feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space in CarRacing-v3 supports both continuous and discrete control modes.\n",
    "\n",
    "In **continuous mode**, the agent outputs three real-valued commands:\n",
    "\n",
    "- steering (ranging from $-1$ to $+1$)\n",
    "- gas\n",
    "- braking\n",
    "\n",
    "In **discrete mode**, the action space is simplified to five actions:\n",
    "\n",
    "- do nothing\n",
    "- steer left\n",
    "- steer right\n",
    "- gas\n",
    "- brake\n",
    "\n",
    "This dual action representation enables a comprehensive evaluation of various RL algorithms under different control settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward structure combines a penalty of $-0.1$ per frame and a reward of $+\\frac{1000}{N}$ for each new track tile visited, where $N$ is the total number of tiles. This incentivizes the agent to balance exploration (visiting tiles) with efficiency (minimizing frame usage). For example, completing the race after visiting all $N$ tiles in 732 frames yields a reward of $1000 - 0.1 \\times 732 = 926.8$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of this project is to compare RL policies across discrete and continuous action modalities. For discrete control, methods like **Deep Q-Network** (DQN) and **SARSA** are implemented, while continuous control is explored using approaches such as the **Cross-Entropy Method** (CEM), **Self-Adaptive Evolution Strategy** (SA-ES), and policy gradient techniques like **Proximal Policy Optimization** (PPO) and **Soft Actor-Critic** (SAC). This comparative analysis aims to understand the strengths and limitations of each method in handling complex decision spaces.\n",
    "\n",
    "The high-dimensional visual inputs in `CarRacing-v3` require effective feature extraction, addressed through a tailored CNN architecture. Transitioning between discrete and continuous action representations also demands careful algorithmic design and parameter tuning to ensure stable learning and convergence. While prior studies have often focused on either discrete or continuous action spaces separately, this work adopts a comparative approach, evaluating different agents within the same environment to assess performance under similar conditions.\n",
    "\n",
    "At this stage, the work outlines the methodology and anticipated challenges, focusing on designing the CNN-based feature extractor, implementing RL algorithms, and establishing a framework for performance comparison. Preliminary findings are yet to be finalized, but the study is expected to provide insights into applying RL in high-dimensional, real-time control tasks. Limitations include the preliminary nature of experiments and the need for further tuning and validation. Future work will involve extensive empirical evaluations, exploring additional policy gradient methods, and refining the network architecture to better handle the complexities of `CarRacing-v3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project's code is available on [GitHub](https://github.com/tr0fin0/ensta_CSC_52081_EP_project), offering a reproducible framework for future investigations and extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Video\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import cast, List, Tuple, Deque, Optional, Callable\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import gymnasium.wrappers as gym_wrap\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PPO import PolicyNetwork, ValueNetwork\n",
    "from SkipFrame import SkipFrame\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_OUTPUT = \"output\"\n",
    "DIRECTORY_MODELS = Path(f\"{DIRECTORY_OUTPUT}/models/\")\n",
    "DIRECTORY_VIDEOS = Path(f\"{DIRECTORY_OUTPUT}/videos/\")\n",
    "\n",
    "if not DIRECTORY_VIDEOS.exists():\n",
    "    DIRECTORY_VIDEOS.mkdir(parents=True)\n",
    "\n",
    "if not DIRECTORY_MODELS.exists():\n",
    "    DIRECTORY_MODELS.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU\n",
    "\n",
    "env = gym.make(\"CarRacing-v3\", \n",
    "               render_mode=\"rgb_array\",\n",
    "               lap_complete_percent=0.95,\n",
    "               continuous=True)\n",
    "\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = gym_wrap.GrayscaleObservation(env)\n",
    "env = gym_wrap.ResizeObservation(env, shape=(84, 84))\n",
    "env = gym_wrap.FrameStackObservation(env, stack_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        device: torch.device,\n",
    "        learning_rate: float = 1e-4,\n",
    "        gamma: float = 0.99,\n",
    "        clip_ratio: float = 0.1,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        lam: float = 0.97,\n",
    "        epochs: int=10,\n",
    "        batch_size: int = 128,\n",
    "        buffer_size: int = 1000,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.epochs = epochs\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.old_log_probs = []\n",
    "\n",
    "        self.policy = PolicyNetwork(\n",
    "            input_dimensions=self.env.observation_space.shape,\n",
    "            output_dim=self.env.action_space.shape[0]\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.value = ValueNetwork(\n",
    "            input_dimensions=self.env.observation_space.shape,\n",
    "            output_dim=1\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=self.learning_rate)\n",
    "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=self.learning_rate)\n",
    "        self.buffer = TensorDictReplayBuffer(\n",
    "            storage=LazyMemmapStorage(self.buffer_size, device=torch.device('cpu'))\n",
    "        )\n",
    "        self.updates = 0\n",
    "\n",
    "    def take_action(self, state):\n",
    "        mu, std = self.policy(state)\n",
    "        distribution = dist.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        action[:, 0] = torch.clamp(action[:, 0], -1.0, 1.0)  # Steering: [-1, 1]\n",
    "        action[:, 1:] = torch.clamp(action[:, 1:], 0.0, 1.0) # Gas and brake: [0, 1]\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)  # Compute log probability\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    \n",
    "    def add_sample(self, state, action, reward, next_state, done, log_prob):\n",
    "        self.buffer.add(\n",
    "            TensorDict({\n",
    "                \"state\": torch.tensor(state),\n",
    "                \"action\": torch.tensor(action),\n",
    "                \"reward\": torch.tensor(reward),\n",
    "                \"next_state\": torch.tensor(next_state),\n",
    "                \"done\": torch.tensor(done),\n",
    "                \"log_prob\": log_prob.detach(),  # Store log_prob in buffer\n",
    "            }, batch_size=[])\n",
    "        )\n",
    "    \n",
    "    def get_samples(self, batch_size:int) -> tuple:\n",
    "        batch = self.buffer.sample(batch_size)\n",
    "        states = batch.get('state').type(torch.FloatTensor).to(self.device)\n",
    "        actions = batch.get('action').squeeze().to(self.device)\n",
    "        rewards = batch.get('reward').squeeze().to(self.device)\n",
    "        next_states = batch.get('next_state').type(torch.FloatTensor).to(self.device)\n",
    "        dones = batch.get('done').squeeze().to(self.device)\n",
    "        log_probs = batch.get('log_prob').squeeze().to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, log_probs\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.updates += 1\n",
    "        states, actions, rewards, next_states, dones, old_log_probs = self.get_samples(self.batch_size)\n",
    "        \n",
    "        next_states = next_states.squeeze(1)\n",
    "        states = states.squeeze(1)\n",
    "\n",
    "        value_loss = self.update_value(states, rewards, next_states, dones)\n",
    "        policy_loss = self.update_policy(states, actions, old_log_probs, rewards, next_states, dones)\n",
    "\n",
    "        return value_loss, policy_loss\n",
    "\n",
    "    def update_value(self, states, rewards, next_states, dones):\n",
    "        with torch.no_grad():\n",
    "            next_values = self.value(next_states)\n",
    "            targets = rewards + self.gamma * next_values * (1 - dones.float())\n",
    "\n",
    "        values = self.value(states)\n",
    "        value_loss = (values - targets).pow(2).mean()\n",
    "\n",
    "        self.optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.value.parameters(), self.max_grad_norm)\n",
    "        self.optimizer_value.step()\n",
    "\n",
    "        return value_loss.item()\n",
    "\n",
    "    def update_policy(self, states, actions, old_log_probs, rewards, next_states, dones):\n",
    "        mu, std = self.policy(states)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        log_probs = distribution.log_prob(actions).sum(dim=-1)  # Compute new log probs\n",
    "        ratio = (log_probs - old_log_probs).exp()  # PPO ratio\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_values = self.value(next_states)\n",
    "            values = self.value(states)\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            gae = 0\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                delta = rewards[t] + self.gamma * next_values[t] * (1 - dones[t].float()) - values[t]\n",
    "                advantages[t] = gae = delta + self.gamma * self.lam * (1 - dones[t].float()) * gae\n",
    "\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages\n",
    "        entropy = distribution.entropy().mean()\n",
    "        policy_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy  # Entropy bonus\n",
    "\n",
    "        self.optimizer_policy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "        self.optimizer_policy.step()\n",
    "\n",
    "        return policy_loss.item()\n",
    "    \n",
    "    def save(self, save_name: str = 'PPO'):\n",
    "        path = DIRECTORY_MODELS / f\"{save_name}_{self.updates}.pt\"\n",
    "        torch.save({\n",
    "            \"policy\": self.policy.state_dict(),\n",
    "            \"value\": self.value.state_dict(),\n",
    "            \"optimizer_policy\": self.optimizer_policy.state_dict(),\n",
    "            \"optimizer_value\": self.optimizer_value.state_dict(),\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path} at update {self.updates}\")\n",
    "    \n",
    "    def load(self, load_name: str):\n",
    "        path = DIRECTORY_MODELS / f\"{load_name}.pt\"\n",
    "        model = torch.load(path)\n",
    "        self.policy.load_state_dict(model[\"policy\"])\n",
    "        self.value.load_state_dict(model[\"value\"])\n",
    "        self.optimizer_policy.load_state_dict(model[\"optimizer_policy\"])\n",
    "        self.optimizer_value.load_state_dict(model[\"optimizer_value\"])\n",
    "        self.learning_rate = model[\"learning_rate\"]\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from output\\models\\PPO_10000.pt\n",
      "Score:-65.52, actions: 250\n",
      "Score:-74.92, actions: 250\n",
      "Score:-74.92, actions: 250\n",
      "Score:-59.25, actions: 250\n",
      "Score:-81.19, actions: 250\n",
      "Average Score for seed 0: -71.16\n",
      "Score:-70.91, actions: 250\n",
      "Score:-74.55, actions: 250\n",
      "Score:-78.18, actions: 250\n",
      "Score:-67.27, actions: 250\n",
      "Score:-52.73, actions: 250\n",
      "Average Score for seed 1: -68.73\n",
      "Score:-73.13, actions: 250\n",
      "Score:-76.12, actions: 250\n",
      "Score:-67.16, actions: 250\n",
      "Score:-79.10, actions: 250\n",
      "Score:-79.10, actions: 250\n",
      "Average Score for seed 2: -74.93\n",
      "Score:-63.10, actions: 250\n",
      "Score:-52.03, actions: 250\n",
      "Score:-77.86, actions: 250\n",
      "Score:-74.17, actions: 250\n",
      "Score:-66.79, actions: 250\n",
      "Average Score for seed 3: -66.79\n",
      "Score:-70.91, actions: 250\n",
      "Score:-78.18, actions: 250\n",
      "Score:-78.18, actions: 250\n",
      "Score:-78.18, actions: 250\n",
      "Score:-63.64, actions: 250\n",
      "Average Score for seed 4: -73.82\n",
      "Score:-81.76, actions: 250\n",
      "Score:-78.72, actions: 250\n",
      "Score:-81.76, actions: 250\n",
      "Score:-75.68, actions: 250\n",
      "Score:-57.45, actions: 250\n",
      "Average Score for seed 5: -75.08\n",
      "Score:-75.35, actions: 250\n",
      "Score:-71.83, actions: 250\n",
      "Score:-82.39, actions: 250\n",
      "Score:-71.83, actions: 250\n",
      "Score:-78.87, actions: 250\n",
      "Average Score for seed 6: -76.06\n",
      "Score:-78.06, actions: 250\n",
      "Score:-74.92, actions: 250\n",
      "Score:-71.79, actions: 250\n",
      "Score:-74.92, actions: 250\n",
      "Score:-74.92, actions: 250\n",
      "Average Score for seed 7: -74.92\n",
      "Score:-52.19, actions: 250\n",
      "Score:-80.08, actions: 250\n",
      "Score:-76.10, actions: 250\n",
      "Score:-60.16, actions: 250\n",
      "Score:-72.11, actions: 250\n",
      "Average Score for seed 8: -68.13\n",
      "Score:-75.44, actions: 250\n",
      "Score:-78.95, actions: 250\n",
      "Score:-78.95, actions: 250\n",
      "Score:-64.91, actions: 250\n",
      "Score:-71.93, actions: 250\n",
      "Average Score for seed 9: -74.04\n",
      "Score:-80.07, actions: 250\n",
      "Score:-76.74, actions: 250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m updating = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m updating:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     next_state, reward, terminated, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(\u001b[32m0\u001b[39m).to(agent.device)\n\u001b[32m     33\u001b[39m     updating = \u001b[38;5;129;01mnot\u001b[39;00m (terminated \u001b[38;5;129;01mor\u001b[39;00m truncated)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:350\u001b[39m, in \u001b[36mRecordVideo.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    348\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    349\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     obs, rew, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28mself\u001b[39m.step_id += \u001b[32m1\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_trigger \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_trigger(\u001b[38;5;28mself\u001b[39m.step_id):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:350\u001b[39m, in \u001b[36mRecordVideo.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    348\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    349\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     obs, rew, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28mself\u001b[39m.step_id += \u001b[32m1\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_trigger \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_trigger(\u001b[38;5;28mself\u001b[39m.step_id):\n",
      "    \u001b[31m[... skipping similar frames: RecordVideo.step at line 350 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:350\u001b[39m, in \u001b[36mRecordVideo.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    348\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    349\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     obs, rew, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28mself\u001b[39m.step_id += \u001b[32m1\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_trigger \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_trigger(\u001b[38;5;28mself\u001b[39m.step_id):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:416\u001b[39m, in \u001b[36mFrameStackObservation.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    407\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    408\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[32m    409\u001b[39m \n\u001b[32m    410\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m \u001b[33;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mself\u001b[39m.obs_queue.append(obs)\n\u001b[32m    419\u001b[39m     updated_obs = deepcopy(\n\u001b[32m    420\u001b[39m         concatenate(\u001b[38;5;28mself\u001b[39m.env.observation_space, \u001b[38;5;28mself\u001b[39m.obs_queue, \u001b[38;5;28mself\u001b[39m.stacked_obs)\n\u001b[32m    421\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\core.py:550\u001b[39m, in \u001b[36mObservationWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    547\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    548\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    549\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observation(observation), reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\core.py:550\u001b[39m, in \u001b[36mObservationWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    547\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    548\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    549\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observation(observation), reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\ensta_CSC_52081_EP_project\\src\\dev\\SkipFrame.py:23\u001b[39m, in \u001b[36mSkipFrame.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     21\u001b[39m total_reward = \u001b[32m0.0\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._skip):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     state, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     total_reward += reward\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\core.py:322\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    319\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    320\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    321\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:563\u001b[39m, in \u001b[36mCarRacing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.world.Step(\u001b[32m1.0\u001b[39m / FPS, \u001b[32m6\u001b[39m * \u001b[32m30\u001b[39m, \u001b[32m2\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m    561\u001b[39m \u001b[38;5;28mself\u001b[39m.t += \u001b[32m1.0\u001b[39m / FPS\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;28mself\u001b[39m.state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_pixels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m step_reward = \u001b[32m0\u001b[39m\n\u001b[32m    566\u001b[39m terminated = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:628\u001b[39m, in \u001b[36mCarRacing._render\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m    625\u001b[39m trans = pygame.math.Vector2((scroll_x, scroll_y)).rotate_rad(angle)\n\u001b[32m    626\u001b[39m trans = (WINDOW_W / \u001b[32m2\u001b[39m + trans[\u001b[32m0\u001b[39m], WINDOW_H / \u001b[32m4\u001b[39m + trans[\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_render_road\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[38;5;28mself\u001b[39m.car.draw(\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m.surf,\n\u001b[32m    631\u001b[39m     zoom,\n\u001b[32m   (...)\u001b[39m\u001b[32m    634\u001b[39m     mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mstate_pixels_list\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstate_pixels\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    635\u001b[39m )\n\u001b[32m    637\u001b[39m \u001b[38;5;28mself\u001b[39m.surf = pygame.transform.flip(\u001b[38;5;28mself\u001b[39m.surf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:672\u001b[39m, in \u001b[36mCarRacing._render_road\u001b[39m\u001b[34m(self, zoom, translation, angle)\u001b[39m\n\u001b[32m    664\u001b[39m field = [\n\u001b[32m    665\u001b[39m     (bounds, bounds),\n\u001b[32m    666\u001b[39m     (bounds, -bounds),\n\u001b[32m    667\u001b[39m     (-bounds, -bounds),\n\u001b[32m    668\u001b[39m     (-bounds, bounds),\n\u001b[32m    669\u001b[39m ]\n\u001b[32m    671\u001b[39m \u001b[38;5;66;03m# draw background\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_draw_colored_polygon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbg_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    674\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;66;03m# draw grass patches\u001b[39;00m\n\u001b[32m    677\u001b[39m grass = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ENSTA\\Avanced_machine_learning\\env-project\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:786\u001b[39m, in \u001b[36mCarRacing._draw_colored_polygon\u001b[39m\u001b[34m(self, surface, poly, color, zoom, translation, angle, clip)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m clip \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    781\u001b[39m     (-MAX_SHAPE_DIM <= coord[\u001b[32m0\u001b[39m] <= WINDOW_W + MAX_SHAPE_DIM)\n\u001b[32m    782\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (-MAX_SHAPE_DIM <= coord[\u001b[32m1\u001b[39m] <= WINDOW_H + MAX_SHAPE_DIM)\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m poly\n\u001b[32m    784\u001b[39m ):\n\u001b[32m    785\u001b[39m     gfxdraw.aapolygon(\u001b[38;5;28mself\u001b[39m.surf, poly, color)\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m     \u001b[43mgfxdraw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilled_polygon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "VIDEO_EVAL = \"PPO_EVALUATION\"\n",
    "(DIRECTORY_VIDEOS / f\"{VIDEO_EVAL}.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=str(DIRECTORY_VIDEOS), name_prefix=VIDEO_EVAL)\n",
    "\n",
    "seeds = [i for i in range(25)]\n",
    "\n",
    "agent = PPO(\n",
    "    env=env,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "MODEL_ID = \"PPO_10000\"\n",
    "agent.load(load_name=MODEL_ID)\n",
    "\n",
    "\n",
    "for episode, seed_id in enumerate(seeds):\n",
    "    score_avg = []\n",
    "\n",
    "    for _ in range(5):\n",
    "        score = 0\n",
    "        action_count = 0\n",
    "        state, info = env.reset(seed=seed_id)\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(agent.device)\n",
    "        action, _ = agent.take_action(state)\n",
    "        action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "        updating = True\n",
    "\n",
    "        while updating:\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(agent.device)\n",
    "\n",
    "            updating = not (terminated or truncated)\n",
    "            score += reward\n",
    "            action_count += 1\n",
    "\n",
    "            state = next_state\n",
    "            action, _ = agent.take_action(state)\n",
    "            action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "\n",
    "        score_avg.append(score)\n",
    "        print(f\"Score:{score:.2f}, actions: {action_count}\")\n",
    "\n",
    "    score_avg = np.mean(score_avg)\n",
    "    print(f\"Average Score for seed {seed_id}: {score_avg:.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(\n",
    "    DIRECTORY_VIDEOS / f\"{VIDEO_EVAL}-episode-0.mp4\",\n",
    "    embed=True,\n",
    "    html_attributes=\"controls autoplay loop\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
