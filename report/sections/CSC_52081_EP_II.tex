\documentclass[../CSC_52081_EP.tex]{subfiles}

\begin{document}
    \section{Background}
    \label{sec:background}

    % Here, tell the reader everything they need to know to understand your work, in your own words. The main emphasis in this section: be pedagogical, target your reader as someone who has followed the course,  and needs to be reminded of relevant concepts to understand what you have done. 

    % You must properly credit any source (textbook, article, course notes/slides) via a suitable reference,  e.g., \cite{RLBook}, Chapter 3, or \cite{Lecture4}, or even a blog post you found on the web \cite{Post3}. 

    % Here is the place to introduce your notation, (e.g., state $s$, policy $\pi_\theta$ parametrized by $\theta$, trajectory $\boldsymbol\tau \sim p_\theta$ from policy $\pi_\theta$ under environment $p$), but make sure each part of your notation is stated somewhere. 

    \subsection{Discrete Action Space}

    DEEP Q-Network (DQN) and SARSA are powerful reinforcement learning algorithms to solve discrete action control problems, suitable to our first approach to the Car Racing environment.
    Both approaches are based on the Q-learning algorithm, which is a model-free reinforcement learning algorithm that aims to learn the optimal action-value function $Q(s, a)$, where $s$ is the state and $a$ is the action.
    However, DQN uses a deep neural network to approximate the Q-function, while SARSA uses a table to store the Q-values \cite{Popular_RL}. To account for the limitations of the SARSA approach in limited high-dimensional state spaces (as the Car Racing environment with an observation space of 96x96x3), we will explore a modern approach called Deep SARSA. \cite{Deep_SARSA}

    Deep SARSA combines the on-policy nature of traditional SARSA with neural network architectures to handle large state spaces effectively. Unlike DQN's off-policy approach, Deep SARSA maintains SARSA's fundamental characteristics while scaling to complex environments.

    On-policy learning methods, such as SARSA, updates the Q-values using actions selected by the current policy. (usually, the $\epsilon$-greedy).
    The $\epsilon$-greedy policy selects a random action (explore) with probability $\epsilon$ and the best action (exploit) with probability 1-$\epsilon$.
    
    On the other hand, off-policy learning methods, such as DQN, updates the Q-values using actions selected by a different policy. (often by greedy selection).

    On-policy methods tend to be more stable, but they can be less sample-efficient and must actively explore during training. In contrast, off-policy methods can learn from experiences, which can lead to better exploration and exploitation of the environment, but may require careful tuning to ensure stability.

    \subsection{Continuous Action Space}

    For the second approach to the Car Racing environment, we will explore algorithms that can handle continuous action spaces.
    Starting from an evolutionary approach, we will test the performance of the Cross-Entropy Method (CEM) in the Car Racing environment.

    The Cross-Entropy Method is a simple optimization algorithm that iteratively samples policies from a Gaussian distribution and updates the distribution parameters to maximize the expected return.
    It generates multiple candidate solutions (policies), ranks them based on performance, and updates the policy distribution using the best-performing candidates. It is efficient in high-dimensional spaces and can discover complex polices through population diversity. It follows the steps:
    
    \begin{enumerate}
        \item Sample $N$ policies from a Gaussian distribution.
        \item Evaluate the policies in the environment.
        \item Select the top $M$ policies.
        \item Update the distribution parameters to fit the selected policies.
        \item Repeat until convergence.
    \end{enumerate}


    Secondly, we will explore and compare two policy-based methods designed for continuous control: Proximal Policy Optimization (PPO) \cite{PPO} and Soft Actor-Critic (SAC) \cite{SAC} in the Car Racing environment.
    
    PPO is primarily an on-policy gradient method because it collects trajectories using the current policy, and updates it using only the data from the most recent rollout (episode or batch). Once data is used for training, it is discarded (unlike fully off-policy algorithms like DQN that store and reuse old experiences). It uses an actor-critic architecture to learn a parameterized policy and, even though PPO directly updates the policy, it still needs a value function (critic network) for advantage estimation (GAE - Generalized Advantage Estimation).
    The advantage function helps reduce variance in the policy updates.
    
    The classic PPO algorithm follows the steps:
    \begin{enumerate}
        \item Collect trajectories using the current policy in a buffer.
        \item Compute the advantage function using the critic network.
        \item Update the policy using the advantage function and the policy gradient.
        \item Repeat until convergence.
    \end{enumerate}


    SAC is an off-policy (meaning it reuses past experiences for learning) actor-critic method that uses the maximum entropy framework to encourage exploration and improve sample efficiency. It learns a stochastic policy that maximizes the expected return while maximizing the entropy of the policy (avoiding premature convergence to suboptimal policies). It follows the steps:
    \begin{enumerate}
        \item Collect trajectories using the current policy.
        \item Compute the advantage function using the critic network.
        \item Update the policy using the advantage function and the policy gradient.
        \item Update the critic network using the temporal difference error.
        \item Repeat until convergence.
    \end{enumerate}

    \subsection{Objective}
    In this study, we will compare the performance of Deep SARSA and DQN in the Car Racing environment to understand the trade-offs between on-policy and off-policy learning methods in the context of discrete action space. Conversely, we also aim to compare the performance of CEM, PPO, and SAC to understand the trade-offs between evolutionary algorithm and policy-based methods in continuous action spaces.

\end{document}
